{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StokedDude/CANSLIM-signal-booster/blob/main/CANSLIM_Booster_V6_CIO_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbTyUIQB0w_V"
      },
      "source": [
        "# üöÄ CANSLIM Signal Booster V6 + Personal CIO System\n",
        "**Complete Trading Intelligence Platform**\n",
        "\n",
        "---\n",
        "\n",
        "## Features:\n",
        "\n",
        "### CANSLIM Signal Booster V6\n",
        "- ‚úÖ Multi-source ingestion (IBD, DeepVue, Primus, Finviz, X, watchlist)\n",
        "- ‚úÖ Convergence scoring (multi-source confirmation)\n",
        "- ‚úÖ Dynamic trust tiers (A/B/C/D auto-classification)\n",
        "- ‚úÖ Context flags (extended/proper setup/volume/timing)\n",
        "- ‚úÖ Instant ticker lookup\n",
        "- ‚úÖ Non-IBD ticker handling (technical-only scoring)\n",
        "\n",
        "### Personal CIO System\n",
        "- üìä Daily Brief generator (pre-market intelligence)\n",
        "- üìà Portfolio health tracker\n",
        "- üéØ Market regime detector\n",
        "- üìâ Trade vs hold advisor\n",
        "- üìã Source performance analytics\n",
        "\n",
        "---\n",
        "\n",
        "**Last Updated:** February 2026  \n",
        "**Version:** 6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB4XVdNH_WRB"
      },
      "source": [
        "## üì¶ 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XzxqMOnz_WRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d09ede3-449b-4c88-ade0-0eb055edf631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Setup complete!\n",
            "üìÖ Current date: 2026-02-03\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q yfinance pandas numpy scipy scikit-learn pyqlib tabulate\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from tabulate import tabulate\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(f\"üìÖ Current date: {datetime.now().strftime('%Y-%m-%d')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah5BaUZ3_WRC"
      },
      "source": [
        "## üìÇ 2. Data Ingestion - Upload IBD Exports\n",
        "\n",
        "**Upload your IBD CSV exports:**\n",
        "- IBD 50\n",
        "- IBD 250\n",
        "- Sector Leaders\n",
        "- Big Cap 20\n",
        "- IPO Leaders\n",
        "- Any other MarketSurge exports\n",
        "\n",
        "**Expected columns:** Ticker, Company, Composite, RS Rating, EPS Rating, SMR, Acc/Dist, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WP4VZGfa_WRD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "4c5cf466-b5da-48d5-f466-d6480a8f89f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ Upload your IBD CSV files (you can upload multiple files)\n",
            "Click 'Choose Files' and select all your IBD exports...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f4905cca-fe95-4f17-aa04-4597d62b4293\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f4905cca-fe95-4f17-aa04-4597d62b4293\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving IBD Big Cap 20.csv to IBD Big Cap 20 (2).csv\n",
            "Saving MarketSurge Growth 250.csv to MarketSurge Growth 250 (2).csv\n",
            "Saving Today's Industry Performance _ NEW HIGHS.csv to Today's Industry Performance _ NEW HIGHS (2).csv\n",
            "Saving Daily % Change.csv to Daily % Change (2).csv\n",
            "Saving IBD 50 Index.csv to IBD 50 Index (2).csv\n",
            "Saving 197 Industry Groups.csv to 197 Industry Groups (2).csv\n",
            "‚úÖ Loaded IBD Big Cap 20 (2).csv: 20 stocks\n",
            "‚úÖ Loaded MarketSurge Growth 250 (2).csv: 299 stocks\n",
            "‚úÖ Loaded Today's Industry Performance _ NEW HIGHS (2).csv: 57 stocks\n",
            "‚úÖ Loaded Daily % Change (2).csv: 197 stocks\n",
            "‚úÖ Loaded IBD 50 Index (2).csv: 50 stocks\n",
            "‚úÖ Loaded 197 Industry Groups (2).csv: 197 stocks\n",
            "\n",
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
            "‚úÖ IBD UNIVERSE LOADED\n",
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
            "Total unique stocks: 547\n",
            "Files processed: 6\n",
            "\n",
            "Sample stocks: FIX, APH, RL, LLY, HEI, KGC, RGLD, NVT, FN, WPM\n",
            "\n",
            "Available data fields: Order, Ticker, Name, Comp_Rating, EPS_Rating, RS_Rating, Ind_Group_RS, SMR_Rating, A/D_Rating, 52-Wk_High, Current_Price, Price_$_Chg, Price_%_Chg, Vol_%_Chg_vs_50-Day, Volume_(1000s), P/E, Sponsor_Rating, Yield_%, Source_File, %_Off_High, 50-Day_Avg_Vol_(1000s), 50-Day_Avg_$_Vol_(1000s), Market_Cap_(mil), Ind_Group_Rank, Industry_Name, IPO_Date, %_New_Highs_in_Group, #_New_Highs_in_Group, Number_of_Stocks, %_Chg_YTD, Ind_Grp_%_Chg_3_Mo, Ind_Grp_Rnk_3_Mo_Ago, Ind_Mkt_Val_(bil), Ind_Grp_Rnk_Last_Week, Ind_Grp_Rnk_6_Mo_Ago\n"
          ]
        }
      ],
      "source": [
        "# Global data storage\n",
        "ibd_universe = pd.DataFrame()\n",
        "all_sources_data = defaultdict(list)\n",
        "ticker_lookup_db = {}\n",
        "\n",
        "# Upload and merge IBD exports\n",
        "print(\"üì§ Upload your IBD CSV files (you can upload multiple files)\")\n",
        "print(\"Click 'Choose Files' and select all your IBD exports...\\n\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "ibd_dataframes = []\n",
        "\n",
        "for filename, content in uploaded.items():\n",
        "    try:\n",
        "        df = pd.read_csv(io.BytesIO(content))\n",
        "\n",
        "        # Standardize column names (handle variations)\n",
        "        df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
        "\n",
        "        # Try to identify ticker column\n",
        "        ticker_col = None\n",
        "        for col in ['Symbol', 'Ticker', 'Stock', 'SYMBOL', 'TICKER']:\n",
        "            if col in df.columns:\n",
        "                ticker_col = col\n",
        "                break\n",
        "\n",
        "        if ticker_col:\n",
        "            df = df.rename(columns={ticker_col: 'Ticker'})\n",
        "            df['Source_File'] = filename\n",
        "            ibd_dataframes.append(df)\n",
        "            print(f\"‚úÖ Loaded {filename}: {len(df)} stocks\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Skipped {filename}: Could not find ticker column\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {filename}: {e}\")\n",
        "\n",
        "# Merge all IBD data\n",
        "if ibd_dataframes:\n",
        "    ibd_universe = pd.concat(ibd_dataframes, ignore_index=True)\n",
        "\n",
        "    # Remove duplicates (keep first occurrence)\n",
        "    ibd_universe = ibd_universe.drop_duplicates(subset='Ticker', keep='first')\n",
        "\n",
        "    # Clean ticker symbols\n",
        "    ibd_universe['Ticker'] = ibd_universe['Ticker'].str.strip().str.upper()\n",
        "\n",
        "    print(f\"\\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
        "    print(f\"‚úÖ IBD UNIVERSE LOADED\")\n",
        "    print(f\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
        "    print(f\"Total unique stocks: {len(ibd_universe)}\")\n",
        "    print(f\"Files processed: {len(ibd_dataframes)}\")\n",
        "    print(f\"\\nSample stocks: {', '.join(ibd_universe['Ticker'].head(10).tolist())}\")\n",
        "\n",
        "    # Show available columns\n",
        "    print(f\"\\nAvailable data fields: {', '.join(ibd_universe.columns.tolist())}\")\n",
        "else:\n",
        "    print(\"‚ùå No IBD data loaded. Please upload CSV files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZIFPa_g_WRD"
      },
      "source": [
        "## üîß 3. Technical Factor Calculations (Qlib-style)\n",
        "\n",
        "Calculate technical factors for all stocks in the universe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5cuZlOHO_WRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98268eee-d3e4-43ec-ba6f-35313e3e682a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Technical Factor Engine initialized\n",
            "Ready to calculate RS, volume patterns, pivot distance, ATR, etc.\n"
          ]
        }
      ],
      "source": [
        "class TechnicalFactorEngine:\n",
        "    \"\"\"Calculate CANSLIM technical factors\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.market_data = {}\n",
        "\n",
        "    def get_price_data(self, ticker: str, period: str = \"6mo\") -> pd.DataFrame:\n",
        "        \"\"\"Fetch price data with caching\"\"\"\n",
        "        if ticker not in self.market_data:\n",
        "            try:\n",
        "                data = yf.download(ticker, period=period, progress=False)\n",
        "                if not data.empty:\n",
        "                    self.market_data[ticker] = data\n",
        "            except:\n",
        "                return pd.DataFrame()\n",
        "        return self.market_data.get(ticker, pd.DataFrame())\n",
        "\n",
        "    def calculate_rs(self, ticker: str, weeks: int = 13) -> float:\n",
        "        \"\"\"Calculate Relative Strength vs SPY\"\"\"\n",
        "        try:\n",
        "            stock_data = self.get_price_data(ticker)\n",
        "            spy_data = self.get_price_data(\"SPY\")\n",
        "\n",
        "            if stock_data.empty or spy_data.empty:\n",
        "                return 50  # Neutral\n",
        "\n",
        "            days = weeks * 5  # Trading days\n",
        "\n",
        "            stock_return = (stock_data['Close'].iloc[-1] / stock_data['Close'].iloc[-days] - 1) * 100\n",
        "            spy_return = (spy_data['Close'].iloc[-1] / spy_data['Close'].iloc[-days] - 1) * 100\n",
        "\n",
        "            # Normalize to 0-99 scale (simplified)\n",
        "            relative_perf = stock_return - spy_return\n",
        "            rs_score = min(99, max(1, 50 + (relative_perf * 2)))\n",
        "\n",
        "            return round(rs_score)\n",
        "        except:\n",
        "            return 50\n",
        "\n",
        "    def calculate_volume_trend(self, ticker: str) -> str:\n",
        "        \"\"\"Analyze volume pattern (Expanding/Contracting/Neutral)\"\"\"\n",
        "        try:\n",
        "            data = self.get_price_data(ticker)\n",
        "            if data.empty or 'Volume' not in data.columns:\n",
        "                return \"Unknown\"\n",
        "\n",
        "            # Compare recent 5 days vs previous 20 days\n",
        "            recent_vol = data['Volume'].iloc[-5:].mean()\n",
        "            avg_vol = data['Volume'].iloc[-25:-5].mean()\n",
        "\n",
        "            if recent_vol > avg_vol * 1.2:\n",
        "                return \"Expanding\"\n",
        "            elif recent_vol < avg_vol * 0.8:\n",
        "                return \"Contracting\"\n",
        "            else:\n",
        "                return \"Neutral\"\n",
        "        except:\n",
        "            return \"Unknown\"\n",
        "\n",
        "    def calculate_pivot_distance(self, ticker: str) -> Tuple[float, str]:\n",
        "        \"\"\"Calculate distance from 52-week high (pivot proxy)\"\"\"\n",
        "        try:\n",
        "            data = self.get_price_data(ticker, period=\"1y\")\n",
        "            if data.empty:\n",
        "                return 0, \"Unknown\"\n",
        "\n",
        "            current_price = data['Close'].iloc[-1]\n",
        "            high_52w = data['High'].max()\n",
        "\n",
        "            distance_pct = ((current_price / high_52w) - 1) * 100\n",
        "\n",
        "            if distance_pct >= -5:\n",
        "                status = \"PROPER SETUP\"\n",
        "            elif distance_pct >= -10:\n",
        "                status = \"NEAR PIVOT\"\n",
        "            elif distance_pct >= -15:\n",
        "                status = \"PULLBACK\"\n",
        "            else:\n",
        "                status = \"EXTENDED DOWN\"\n",
        "\n",
        "            return round(distance_pct, 1), status\n",
        "        except:\n",
        "            return 0, \"Unknown\"\n",
        "\n",
        "    def calculate_atr(self, ticker: str, period: int = 14) -> float:\n",
        "        \"\"\"Calculate Average True Range for volatility/stop placement\"\"\"\n",
        "        try:\n",
        "            data = self.get_price_data(ticker)\n",
        "            if data.empty:\n",
        "                return 0\n",
        "\n",
        "            high_low = data['High'] - data['Low']\n",
        "            high_close = abs(data['High'] - data['Close'].shift())\n",
        "            low_close = abs(data['Low'] - data['Close'].shift())\n",
        "\n",
        "            true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "            atr = true_range.rolling(window=period).mean().iloc[-1]\n",
        "\n",
        "            return round(atr, 2)\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    def determine_timeframe(self, atr: float, pivot_distance: float) -> str:\n",
        "        \"\"\"Determine if stock is suitable for daytrade/swing/position\"\"\"\n",
        "        if atr > 5 and abs(pivot_distance) < 5:\n",
        "            return \"DAYTRADE\"\n",
        "        elif abs(pivot_distance) < 5:\n",
        "            return \"SWING\"\n",
        "        elif abs(pivot_distance) < 10:\n",
        "            return \"POSITION\"\n",
        "        else:\n",
        "            return \"WATCH\"\n",
        "\n",
        "# Initialize engine\n",
        "tech_engine = TechnicalFactorEngine()\n",
        "\n",
        "print(\"‚úÖ Technical Factor Engine initialized\")\n",
        "print(\"Ready to calculate RS, volume patterns, pivot distance, ATR, etc.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHNvWsgg_WRE"
      },
      "source": [
        "## üéØ 4. Process IBD Universe - Add Technical Overlays"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def overlay_technical_factors(\n",
        "    ibd_universe: pd.DataFrame,\n",
        "    tech_engine: Any,\n",
        "    chunk_size: int = 50,\n",
        "    max_workers: int = 12,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add technical factor columns to an IBD universe DataFrame.\n",
        "\n",
        "    Improvements vs original:\n",
        "    - Faster row iteration (itertuples)\n",
        "    - Parallel per-ticker computation (ThreadPoolExecutor)\n",
        "    - Per-ticker error isolation (one failure won't break the run)\n",
        "    - Adds Status/Error columns so you can audit failures\n",
        "    \"\"\"\n",
        "\n",
        "    if ibd_universe is None or ibd_universe.empty:\n",
        "        print(\"‚ùå No IBD universe to process. Please upload IBD exports first.\")\n",
        "        return ibd_universe\n",
        "\n",
        "    if \"Ticker\" not in ibd_universe.columns:\n",
        "        raise ValueError(\"ibd_universe must contain a 'Ticker' column\")\n",
        "\n",
        "    print(\"üîÑ Processing IBD universe with technical factors...\")\n",
        "    print(f\"Analyzing {len(ibd_universe)} stocks\\n\")\n",
        "\n",
        "    # Pre-fetch benchmark once (warm caches, speed up RS calculations)\n",
        "    # Assumes tech_engine caches internally; if not, this is still harmless.\n",
        "    _ = tech_engine.get_price_data(\"SPY\")\n",
        "\n",
        "    total_stocks = len(ibd_universe)\n",
        "    tickers = ibd_universe[\"Ticker\"].astype(str).str.upper().tolist()\n",
        "\n",
        "    def compute_one(ticker: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Compute factors for one ticker.\n",
        "        This is isolated so exceptions are captured per ticker.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            rs_calc = tech_engine.calculate_rs(ticker)\n",
        "            volume_trend = tech_engine.calculate_volume_trend(ticker)\n",
        "            pivot_dist, pivot_status = tech_engine.calculate_pivot_distance(ticker)\n",
        "            atr = tech_engine.calculate_atr(ticker)\n",
        "            timeframe = tech_engine.determine_timeframe(atr, pivot_dist)\n",
        "\n",
        "            return {\n",
        "                \"Ticker\": ticker,\n",
        "                \"RS_Calculated\": rs_calc,\n",
        "                \"Volume_Trend\": volume_trend,\n",
        "                \"Pivot_Distance_%\": pivot_dist,\n",
        "                \"Pivot_Status\": pivot_status,\n",
        "                \"ATR\": atr,\n",
        "                \"Timeframe\": timeframe,\n",
        "                \"Status\": \"OK\",\n",
        "                \"Error\": \"\",\n",
        "            }\n",
        "        except Exception as e:\n",
        "            # Keep row so merges are stable; you can later filter Status != OK\n",
        "            return {\n",
        "                \"Ticker\": ticker,\n",
        "                \"RS_Calculated\": None,\n",
        "                \"Volume_Trend\": None,\n",
        "                \"Pivot_Distance_%\": None,\n",
        "                \"Pivot_Status\": None,\n",
        "                \"ATR\": None,\n",
        "                \"Timeframe\": None,\n",
        "                \"Status\": \"FAIL\",\n",
        "                \"Error\": f\"{type(e).__name__}: {e}\",\n",
        "            }\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Process in chunks to provide progress feedback without spamming\n",
        "    for start in range(0, total_stocks, chunk_size):\n",
        "        chunk_tickers = tickers[start : start + chunk_size]\n",
        "\n",
        "        # Threaded execution is best when tech_engine does network/disk I/O per ticker.\n",
        "        # If your tech_engine is pure CPU, use ProcessPoolExecutor instead.\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "            futures = {ex.submit(compute_one, t): t for t in chunk_tickers}\n",
        "            for fut in as_completed(futures):\n",
        "                results.append(fut.result())\n",
        "\n",
        "        processed = min(start + chunk_size, total_stocks)\n",
        "        pct = (processed / total_stocks) * 100\n",
        "        print(f\"Progress: {processed}/{total_stocks} ({pct:.1f}%)\")\n",
        "\n",
        "    technical_df = pd.DataFrame(results)\n",
        "\n",
        "    # Merge technical overlay back into IBD universe\n",
        "    out = ibd_universe.copy()\n",
        "    out[\"Ticker\"] = out[\"Ticker\"].astype(str).str.upper()\n",
        "    out = out.merge(technical_df, on=\"Ticker\", how=\"left\")\n",
        "\n",
        "    print(\"\\n‚úÖ Technical overlay complete!\")\n",
        "    print(f\"Total stocks processed: {len(out)}\")\n",
        "    print(f\"Failures: {(out['Status'] == 'FAIL').sum() if 'Status' in out.columns else 0}\")\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "sX_wHY2xJElZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-loOo7ZT_WRF"
      },
      "source": [
        "## üèÜ 5. Tier Classification System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SSckesUV_WRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "987ceb45-907c-459b-a348-57df32febea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
            "üìä TIER CLASSIFICATION\n",
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
            "A-TIER: 0 stocks\n",
            "B-TIER: 0 stocks\n",
            "C-TIER: 346 stocks\n",
            "D-TIER: 201 stocks\n",
            "\n",
            "‚úÖ Tier classification complete!\n",
            "‚úÖ Lookup database built with 547 stocks\n"
          ]
        }
      ],
      "source": [
        "def classify_tier(row: pd.Series) -> str:\n",
        "    \"\"\"Assign A/B/C/D tier based on CANSLIM criteria\"\"\"\n",
        "\n",
        "    # Get RS rating (prefer IBD's if available, otherwise use calculated)\n",
        "    rs_ibd_cols = ['RS_Rating', 'RS', 'Relative_Strength']\n",
        "    rs = None\n",
        "    for col in rs_ibd_cols:\n",
        "        if col in row.index and pd.notna(row[col]):\n",
        "            rs = row[col]\n",
        "            break\n",
        "    if rs is None:\n",
        "        rs = row.get('RS_Calculated', 50)\n",
        "\n",
        "    # Get Composite rating\n",
        "    comp_cols = ['Composite', 'Composite_Rating', 'Comp']\n",
        "    composite = None\n",
        "    for col in comp_cols:\n",
        "        if col in row.index and pd.notna(row[col]):\n",
        "            composite = row[col]\n",
        "            break\n",
        "    if composite is None:\n",
        "        composite = 50\n",
        "\n",
        "    pivot_status = row.get('Pivot_Status', 'Unknown')\n",
        "\n",
        "    # A-TIER: IBD 50 quality + proper setup\n",
        "    if rs >= 90 and composite >= 90 and pivot_status == \"PROPER SETUP\":\n",
        "        return \"A-TIER\"\n",
        "\n",
        "    # B-TIER: IBD 250 quality OR strong technical\n",
        "    elif rs >= 80 and composite >= 80:\n",
        "        return \"B-TIER\"\n",
        "\n",
        "    # C-TIER: Moderate quality\n",
        "    elif rs >= 70 or composite >= 70:\n",
        "        return \"C-TIER\"\n",
        "\n",
        "    # D-TIER: Low quality\n",
        "    else:\n",
        "        return \"D-TIER\"\n",
        "\n",
        "if not ibd_universe.empty:\n",
        "    # Apply tier classification\n",
        "    ibd_universe['Tier'] = ibd_universe.apply(classify_tier, axis=1)\n",
        "\n",
        "    # Add IBD verification flag\n",
        "    ibd_universe['Verification'] = \"IBD-VERIFIED\"\n",
        "\n",
        "    # Summary stats\n",
        "    tier_counts = ibd_universe['Tier'].value_counts()\n",
        "\n",
        "    print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
        "    print(\"üìä TIER CLASSIFICATION\")\n",
        "    print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
        "    for tier in ['A-TIER', 'B-TIER', 'C-TIER', 'D-TIER']:\n",
        "        count = tier_counts.get(tier, 0)\n",
        "        print(f\"{tier}: {count} stocks\")\n",
        "\n",
        "    print(\"\\n‚úÖ Tier classification complete!\")\n",
        "\n",
        "    # Build lookup database\n",
        "    for idx, row in ibd_universe.iterrows():\n",
        "        ticker_lookup_db[row['Ticker']] = row.to_dict()\n",
        "\n",
        "    print(f\"‚úÖ Lookup database built with {len(ticker_lookup_db)} stocks\")\n",
        "else:\n",
        "    print(\"‚ùå No data to classify\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utEOJBc-_WRF"
      },
      "source": [
        "## üì• 6. Multi-Source Input - Other Idea Sources\n",
        "\n",
        "Enter tickers from other sources (DeepVue, Primus, Finviz, X mentions, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPbG66iA_WRG",
        "outputId": "fa5116b6-35df-4bef-fc46-5a4f656465be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "üì• ADD TICKERS FROM OTHER SOURCES\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "Enter tickers from each source (comma or space separated)\n",
            "Leave blank and press Enter to skip\n",
            "\n",
            "üîπ DeepVue True Market Leaders: GE\n",
            "   Added 1 tickers from DeepVue\n",
            "üîπ Primus Pre-Market Scan: \n",
            "üîπ Finviz Scan Results: TER SNDK PLTR DVA LITE\n",
            "   Added 5 tickers from Finviz\n",
            "üîπ X/Twitter Mentions: \n",
            "üîπ Personal Watchlist: \n",
            "\n",
            "‚úÖ Total unique tickers from all sources: 6\n"
          ]
        }
      ],
      "source": [
        "def add_source_tickers(source_name: str, tickers_input: str):\n",
        "    \"\"\"Add tickers from a source\"\"\"\n",
        "    # Parse ticker input (comma or space separated)\n",
        "    tickers = [t.strip().upper() for t in tickers_input.replace(',', ' ').split() if t.strip()]\n",
        "\n",
        "    for ticker in tickers:\n",
        "        all_sources_data[ticker].append(source_name)\n",
        "\n",
        "    return tickers\n",
        "\n",
        "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
        "print(\"üì• ADD TICKERS FROM OTHER SOURCES\")\n",
        "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
        "print(\"\\nEnter tickers from each source (comma or space separated)\")\n",
        "print(\"Leave blank and press Enter to skip\\n\")\n",
        "\n",
        "# DeepVue\n",
        "deepvue_input = input(\"üîπ DeepVue True Market Leaders: \")\n",
        "if deepvue_input:\n",
        "    deepvue_tickers = add_source_tickers(\"DeepVue\", deepvue_input)\n",
        "    print(f\"   Added {len(deepvue_tickers)} tickers from DeepVue\")\n",
        "\n",
        "# Primus\n",
        "primus_input = input(\"üîπ Primus Pre-Market Scan: \")\n",
        "if primus_input:\n",
        "    primus_tickers = add_source_tickers(\"Primus\", primus_input)\n",
        "    print(f\"   Added {len(primus_tickers)} tickers from Primus\")\n",
        "\n",
        "# Finviz\n",
        "finviz_input = input(\"üîπ Finviz Scan Results: \")\n",
        "if finviz_input:\n",
        "    finviz_tickers = add_source_tickers(\"Finviz\", finviz_input)\n",
        "    print(f\"   Added {len(finviz_tickers)} tickers from Finviz\")\n",
        "\n",
        "# X/Twitter mentions\n",
        "x_input = input(\"üîπ X/Twitter Mentions: \")\n",
        "if x_input:\n",
        "    x_tickers = add_source_tickers(\"X_Mentions\", x_input)\n",
        "    print(f\"   Added {len(x_tickers)} tickers from X\")\n",
        "\n",
        "# Personal watchlist\n",
        "watchlist_input = input(\"üîπ Personal Watchlist: \")\n",
        "if watchlist_input:\n",
        "    watchlist_tickers = add_source_tickers(\"Watchlist\", watchlist_input)\n",
        "    print(f\"   Added {len(watchlist_tickers)} tickers from Watchlist\")\n",
        "\n",
        "# Summary\n",
        "total_unique = len(all_sources_data)\n",
        "print(f\"\\n‚úÖ Total unique tickers from all sources: {total_unique}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7CnH_j-_WRG"
      },
      "source": [
        "## ‚≠ê 7. Convergence Scoring Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Kug6VV0j_WRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26278f08-10e0-432e-e8a8-962e3deb02bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
            "‚≠ê CONVERGENCE SCORING\n",
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
            "Total tickers analyzed: 550\n",
            "High conviction (‚≠ê‚≠ê‚≠ê): 0\n",
            "Medium conviction (‚≠ê‚≠ê): 0\n",
            "Low conviction (‚≠ê): 550\n",
            "\n",
            "‚úÖ Convergence analysis complete!\n"
          ]
        }
      ],
      "source": [
        "# Source weights (customize as needed)\n",
        "SOURCE_WEIGHTS = {\n",
        "    'IBD_50': 3,\n",
        "    'IBD_250': 2,\n",
        "    'DeepVue': 2,\n",
        "    'Primus': 2,\n",
        "    'Finviz': 1,\n",
        "    'X_Mentions': 1,\n",
        "    'Watchlist': 1\n",
        "}\n",
        "\n",
        "def calculate_convergence_score(ticker: str, sources: List[str]) -> Tuple[int, str]:\n",
        "    \"\"\"Calculate weighted convergence score\"\"\"\n",
        "    total_weight = sum(SOURCE_WEIGHTS.get(source, 1) for source in sources)\n",
        "    max_possible = sum(SOURCE_WEIGHTS.values())\n",
        "\n",
        "    # Normalize to 0-10 scale\n",
        "    score = min(10, int((total_weight / max_possible) * 10))\n",
        "\n",
        "    # Stars for display\n",
        "    if score >= 8:\n",
        "        stars = \"‚≠ê‚≠ê‚≠ê\"\n",
        "    elif score >= 6:\n",
        "        stars = \"‚≠ê‚≠ê\"\n",
        "    elif score >= 4:\n",
        "        stars = \"‚≠ê\"\n",
        "    else:\n",
        "        stars = \"\"\n",
        "\n",
        "    return score, stars\n",
        "\n",
        "# Add IBD tickers to sources\n",
        "if not ibd_universe.empty:\n",
        "    for ticker in ibd_universe['Ticker'].tolist():\n",
        "        # Determine which IBD list (simplified)\n",
        "        row = ibd_universe[ibd_universe['Ticker'] == ticker].iloc[0]\n",
        "\n",
        "        # Check RS to determine IBD 50 vs 250\n",
        "        rs_cols = ['RS_Rating', 'RS', 'Relative_Strength']\n",
        "        rs = None\n",
        "        for col in rs_cols:\n",
        "            if col in row.index and pd.notna(row[col]):\n",
        "                rs = row[col]\n",
        "                break\n",
        "\n",
        "        if rs and rs >= 90:\n",
        "            all_sources_data[ticker].append('IBD_50')\n",
        "        else:\n",
        "            all_sources_data[ticker].append('IBD_250')\n",
        "\n",
        "# Calculate convergence for all tickers\n",
        "convergence_data = []\n",
        "\n",
        "for ticker, sources in all_sources_data.items():\n",
        "    score, stars = calculate_convergence_score(ticker, sources)\n",
        "\n",
        "    convergence_data.append({\n",
        "        'Ticker': ticker,\n",
        "        'Sources': ', '.join(sources),\n",
        "        'Source_Count': len(sources),\n",
        "        'Convergence_Score': score,\n",
        "        'Stars': stars\n",
        "    })\n",
        "\n",
        "convergence_df = pd.DataFrame(convergence_data).sort_values('Convergence_Score', ascending=False)\n",
        "\n",
        "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
        "print(\"‚≠ê CONVERGENCE SCORING\")\n",
        "print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
        "print(f\"Total tickers analyzed: {len(convergence_df)}\")\n",
        "print(f\"High conviction (‚≠ê‚≠ê‚≠ê): {len(convergence_df[convergence_df['Convergence_Score'] >= 8])}\")\n",
        "print(f\"Medium conviction (‚≠ê‚≠ê): {len(convergence_df[convergence_df['Convergence_Score'].between(6, 7)])}\")\n",
        "print(f\"Low conviction (‚≠ê): {len(convergence_df[convergence_df['Convergence_Score'] < 6])}\")\n",
        "\n",
        "print(\"\\n‚úÖ Convergence analysis complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OX9Pbyr_WRG"
      },
      "source": [
        "## üîç 8. Instant Ticker Lookup Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWyx2o9n_WRG"
      },
      "source": [
        "## üìä 9. Daily Brief Generator - Personal CIO System"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from typing import Any, Dict\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def lookup_ticker(ticker: str):\n",
        "    \"\"\"\n",
        "    Instant ticker quality lookup.\n",
        "\n",
        "    Assumes these exist in the outer scope:\n",
        "      - ticker_lookup_db: dict[str, dict]\n",
        "      - convergence_df: pd.DataFrame\n",
        "      - tech_engine: object with calculate_rs / calculate_volume_trend / calculate_pivot_distance / calculate_atr\n",
        "      - pd imported as pandas\n",
        "    \"\"\"\n",
        "    ticker = str(ticker).strip().upper()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(f\"üìä TICKER LOOKUP: {ticker}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Helper: safe convergence lookup (avoids KeyError / iloc crashes)\n",
        "    def _conv_row(t: str) -> pd.DataFrame:\n",
        "        \"\"\"Return convergence rows for ticker, or empty df if not available.\"\"\"\n",
        "        if convergence_df is None or convergence_df.empty or \"Ticker\" not in convergence_df.columns:\n",
        "            return pd.DataFrame()\n",
        "        # Normalize case for matching\n",
        "        return convergence_df[convergence_df[\"Ticker\"].astype(str).str.upper() == t]\n",
        "\n",
        "    def _safe_first(df: pd.DataFrame, col: str, default: Any = \"\") -> Any:\n",
        "        \"\"\"Return first cell in df[col] if possible; otherwise default.\"\"\"\n",
        "        if df is None or df.empty or col not in df.columns:\n",
        "            return default\n",
        "        val = df[col].iloc[0]\n",
        "        return default if pd.isna(val) else val\n",
        "\n",
        "    # Check if in IBD database\n",
        "    if ticker in ticker_lookup_db:\n",
        "        data: Dict[str, Any] = ticker_lookup_db[ticker]\n",
        "\n",
        "        # Convergence info\n",
        "        conv_info = _conv_row(ticker)\n",
        "        sources = _safe_first(conv_info, \"Sources\", default=\"IBD only\")\n",
        "        stars = _safe_first(conv_info, \"Stars\", default=\"\")\n",
        "\n",
        "        tier = data.get(\"Tier\", \"UNKNOWN\")\n",
        "        print(f\"\\n‚úÖ {tier} - IBD VERIFIED\")\n",
        "        print(f\"\\nConviction: {stars}\")\n",
        "        print(f\"Sources: {sources}\")\n",
        "\n",
        "        # Ratings\n",
        "        print(\"\\nüìà CANSLIM Ratings:\")\n",
        "        for col in [\"Composite\", \"RS_Rating\", \"EPS_Rating\", \"SMR\", \"Acc/Dist\"]:\n",
        "            if col in data and pd.notna(data[col]):\n",
        "                print(f\"  {col}: {data[col]}\")\n",
        "\n",
        "        # Technical\n",
        "        print(\"\\nüìä Technical Analysis:\")\n",
        "        print(f\"  Pivot Status: {data.get('Pivot_Status', 'N/A')}\")\n",
        "        print(f\"  Distance from Pivot: {data.get('Pivot_Distance_%', 'N/A')}%\")\n",
        "        print(f\"  Volume Trend: {data.get('Volume_Trend', 'N/A')}\")\n",
        "        print(f\"  ATR: ${data.get('ATR', 'N/A')}\")\n",
        "        print(f\"  Timeframe: {data.get('Timeframe', 'N/A')}\")\n",
        "\n",
        "        # Recommendation\n",
        "        pivot_status = str(data.get(\"Pivot_Status\", \"\")).upper()\n",
        "\n",
        "        print(\"\\nüí° Recommendation:\")\n",
        "        if tier == \"A-TIER\" and pivot_status == \"PROPER SETUP\":\n",
        "            print(\"  üü¢ HIGH CONVICTION - Ready to trade\")\n",
        "        elif tier == \"A-TIER\":\n",
        "            print(\"  üü° QUALITY STOCK - Wait for better entry\")\n",
        "        elif tier == \"B-TIER\" and pivot_status == \"PROPER SETUP\":\n",
        "            print(\"  üü° WATCHABLE - Consider smaller position\")\n",
        "        elif tier == \"B-TIER\":\n",
        "            print(\"  üü° WATCH - Set alerts for entry\")\n",
        "        else:\n",
        "            print(\"  üî¥ PASS - Low conviction\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  NOT IN IBD DATABASE\")\n",
        "\n",
        "        # Check if in other sources\n",
        "        conv_info = _conv_row(ticker)\n",
        "\n",
        "        if not conv_info.empty:\n",
        "            sources = _safe_first(conv_info, \"Sources\", default=\"Other sources\")\n",
        "            print(f\"\\nFound in: {sources}\")\n",
        "            print(\"\\nCalculating technical scores...\")\n",
        "\n",
        "            # Calculate technical-only scores (wrap to avoid one ticker killing the function)\n",
        "            try:\n",
        "                rs = tech_engine.calculate_rs(ticker)\n",
        "                volume = tech_engine.calculate_volume_trend(ticker)\n",
        "                pivot_dist, pivot_status = tech_engine.calculate_pivot_distance(ticker)\n",
        "                atr = tech_engine.calculate_atr(ticker)\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ùå Technical calc failed: {type(e).__name__}: {e}\")\n",
        "                print(\"\\n\" + \"=\" * 50)\n",
        "                return\n",
        "\n",
        "            print(\"\\nüìä TECHNICAL-ONLY Analysis:\")\n",
        "            print(f\"  RS (estimated): {rs}\")\n",
        "            print(f\"  Volume: {volume}\")\n",
        "            print(f\"  Pivot Status: {pivot_status} ({pivot_dist}%)\")\n",
        "            print(f\"  ATR: ${atr}\")\n",
        "\n",
        "            # Grade\n",
        "            pivot_status_u = str(pivot_status).upper()\n",
        "            if rs >= 80 and pivot_status_u == \"PROPER SETUP\":\n",
        "                grade = \"B-TIER (Technical-only)\"\n",
        "                rec = \"üü° WATCHABLE - No fundamental verification, smaller size\"  # <-- fixed extra ')'\n",
        "            elif rs >= 70:\n",
        "                grade = \"C-TIER (Technical-only)\"\n",
        "                rec = \"üü† DAYTRADE ONLY - Tight stops, no fundamentals\"\n",
        "            else:\n",
        "                grade = \"D-TIER\"\n",
        "                rec = \"üî¥ PASS - Weak on all metrics\"\n",
        "\n",
        "            print(f\"\\nGrade: {grade}\")\n",
        "            print(f\"\\nüí° Recommendation: {rec}\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Not found in any source\")\n",
        "            print(\"Consider adding to watchlist or ignoring\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "\n",
        "# Example usage (keep this out of libraries/modules if you plan to import it elsewhere)\n",
        "print(\"\\nüîç INSTANT TICKER LOOKUP\")\n",
        "print(\"Enter any ticker to get instant quality report\")\n",
        "print(\"Example: lookup_ticker('NVDA')\\n\")\n",
        "\n",
        "test_ticker = input(\"Enter ticker to lookup (or press Enter to skip): \").strip()\n",
        "if test_ticker:\n",
        "    lookup_ticker(test_ticker)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf3kEj9Ct8k0",
        "outputId": "def3c63e-9c04-481b-f344-693e1be7ae9c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç INSTANT TICKER LOOKUP\n",
            "Enter any ticker to get instant quality report\n",
            "Example: lookup_ticker('NVDA')\n",
            "\n",
            "Enter ticker to lookup (or press Enter to skip): NVDA\n",
            "\n",
            "==================================================\n",
            "üìä TICKER LOOKUP: NVDA\n",
            "==================================================\n",
            "\n",
            "‚úÖ C-TIER - IBD VERIFIED\n",
            "\n",
            "Conviction: \n",
            "Sources: IBD_250\n",
            "\n",
            "üìà CANSLIM Ratings:\n",
            "  RS_Rating: 84.0\n",
            "  EPS_Rating: 99.0\n",
            "\n",
            "üìä Technical Analysis:\n",
            "  Pivot Status: N/A\n",
            "  Distance from Pivot: N/A%\n",
            "  Volume Trend: N/A\n",
            "  ATR: $N/A\n",
            "  Timeframe: N/A\n",
            "\n",
            "üí° Recommendation:\n",
            "  üî¥ PASS - Low conviction\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def generate_daily_brief(\n",
        "    top_n_daytrades: int = 5,\n",
        "    top_n_swings: int = 3,\n",
        "    *,\n",
        "    tech_engine: Any,\n",
        "    ibd_universe: Optional[pd.DataFrame] = None,\n",
        "    convergence_df: Optional[pd.DataFrame] = None,\n",
        "    all_sources_data: Optional[Dict[str, Any]] = None,\n",
        "    ticker_lookup_db: Optional[Dict[str, Any]] = None,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generate comprehensive pre-market daily brief.\n",
        "\n",
        "    Fixes vs your version:\n",
        "    - Imports datetime\n",
        "    - No bare except; prints the exception type/message\n",
        "    - Handles missing/empty dataframes\n",
        "    - Safe merges (only if required columns exist)\n",
        "    - Avoids KeyError on missing columns\n",
        "    - Handles missing globals by injecting dependencies\n",
        "    \"\"\"\n",
        "\n",
        "    current_date = datetime.now().strftime(\"%A, %B %d, %Y\")\n",
        "\n",
        "    # Normalize optional inputs\n",
        "    ibd_universe = ibd_universe if isinstance(ibd_universe, pd.DataFrame) else pd.DataFrame()\n",
        "    convergence_df = convergence_df if isinstance(convergence_df, pd.DataFrame) else pd.DataFrame()\n",
        "    all_sources_data = all_sources_data or {}\n",
        "    ticker_lookup_db = ticker_lookup_db or {}\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üìã DAILY TRADING BRIEF\")\n",
        "    print(f\"{current_date} | Pre-Market\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Market Regime (simple proxy)\n",
        "    # ---------------------------\n",
        "    try:\n",
        "        spy_data = tech_engine.get_price_data(\"SPY\", period=\"1mo\")\n",
        "        if spy_data is None or len(spy_data) < 6 or \"Close\" not in spy_data.columns:\n",
        "            raise ValueError(\"SPY data missing or insufficient (need >=6 rows with 'Close')\")\n",
        "\n",
        "        spy_return_5d = ((spy_data[\"Close\"].iloc[-1] / spy_data[\"Close\"].iloc[-5]) - 1) * 100\n",
        "\n",
        "        vix_data = tech_engine.get_price_data(\"^VIX\", period=\"1mo\")\n",
        "        if vix_data is not None and not vix_data.empty and \"Close\" in vix_data.columns:\n",
        "            vix_current = float(vix_data[\"Close\"].iloc[-1])\n",
        "        else:\n",
        "            vix_current = 15.0  # fallback default\n",
        "\n",
        "        if spy_return_5d > 1 and vix_current < 15:\n",
        "            regime = \"Trending Up ‚úÖ\"\n",
        "            sizing = \"Full position sizing OK\"\n",
        "        elif spy_return_5d < -1 or vix_current > 20:\n",
        "            regime = \"Choppy / Distribution ‚ö†Ô∏è\"\n",
        "            sizing = \"Reduce size 50%, selective trades only\"\n",
        "        else:\n",
        "            regime = \"Neutral\"\n",
        "            sizing = \"Normal position sizing\"\n",
        "\n",
        "        print(f\"\\nMARKET REGIME: {regime}\")\n",
        "        print(f\"Recommendation: {sizing}\")\n",
        "        print(f\"SPY (5-day): {spy_return_5d:+.1f}%\")\n",
        "        print(f\"VIX: {vix_current:.1f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nMARKET REGIME: Unable to fetch (check connection)\")\n",
        "        print(f\"Reason: {type(e).__name__}: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"‚îÅ\" * 60)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Helpers for safe operations\n",
        "    # ---------------------------\n",
        "    def has_cols(df: pd.DataFrame, cols: list[str]) -> bool:\n",
        "        \"\"\"Return True if df contains all required columns.\"\"\"\n",
        "        return isinstance(df, pd.DataFrame) and all(c in df.columns for c in cols)\n",
        "\n",
        "    def safe_merge_convergence(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Merge convergence fields if available.\n",
        "        Avoids KeyErrors when columns are missing.\n",
        "        \"\"\"\n",
        "        needed = [\"Ticker\", \"Stars\", \"Sources\", \"Convergence_Score\"]\n",
        "        if df.empty:\n",
        "            return df\n",
        "\n",
        "        if not has_cols(df, [\"Ticker\"]):\n",
        "            return df\n",
        "\n",
        "        if has_cols(convergence_df, needed):\n",
        "            return df.merge(convergence_df[needed], on=\"Ticker\", how=\"left\")\n",
        "\n",
        "        # If convergence_df exists but missing some columns, merge only what exists\n",
        "        if not convergence_df.empty and \"Ticker\" in convergence_df.columns:\n",
        "            available = [\"Ticker\"] + [c for c in [\"Stars\", \"Sources\", \"Convergence_Score\"] if c in convergence_df.columns]\n",
        "            if len(available) > 1:\n",
        "                return df.merge(convergence_df[available], on=\"Ticker\", how=\"left\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def safe_sort(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Sort by convergence then tier if possible; otherwise return df unchanged.\"\"\"\n",
        "        if df.empty:\n",
        "            return df\n",
        "\n",
        "        sort_cols = []\n",
        "        ascending = []\n",
        "\n",
        "        if \"Convergence_Score\" in df.columns:\n",
        "            sort_cols.append(\"Convergence_Score\")\n",
        "            ascending.append(False)\n",
        "\n",
        "        if \"Tier\" in df.columns:\n",
        "            sort_cols.append(\"Tier\")\n",
        "            # Your original used ascending True for Tier; keep it\n",
        "            ascending.append(True)\n",
        "\n",
        "        if sort_cols:\n",
        "            return df.sort_values(sort_cols, ascending=ascending)\n",
        "\n",
        "        return df\n",
        "\n",
        "    # ---------------------------\n",
        "    # Top Daytrades\n",
        "    # ---------------------------\n",
        "    print(f\"\\nüéØ TOP {top_n_daytrades} DAYTRADE SETUPS\\n\")\n",
        "\n",
        "    if not ibd_universe.empty and has_cols(ibd_universe, [\"Timeframe\", \"Tier\", \"Ticker\"]):\n",
        "        daytrade_candidates = ibd_universe[\n",
        "            (ibd_universe[\"Timeframe\"] == \"DAYTRADE\")\n",
        "            & (ibd_universe[\"Tier\"].isin([\"A-TIER\", \"B-TIER\"]))\n",
        "        ].copy()\n",
        "\n",
        "        daytrade_candidates = safe_merge_convergence(daytrade_candidates)\n",
        "        daytrade_candidates = safe_sort(daytrade_candidates).head(top_n_daytrades)\n",
        "\n",
        "        if daytrade_candidates.empty:\n",
        "            print(\"   No daytrade candidates matching filters\\n\")\n",
        "        else:\n",
        "            for i, row in enumerate(daytrade_candidates.itertuples(index=False), 1):\n",
        "                rowd = row._asdict()  # convert namedtuple to dict for .get usage\n",
        "\n",
        "                rs = rowd.get(\"RS_Rating\", rowd.get(\"RS_Calculated\", \"N/A\"))\n",
        "                comp = rowd.get(\"Composite\", \"N/A\")\n",
        "\n",
        "                print(f\"{i}. {rowd.get('Ticker')} - {rowd.get('Tier')} {rowd.get('Stars', '')}\")\n",
        "                print(f\"   Sources: {rowd.get('Sources', 'IBD only')}\")\n",
        "                print(f\"   RS: {rs} | Composite: {comp}\")\n",
        "                print(f\"   Setup: {rowd.get('Pivot_Status', 'N/A')} ({rowd.get('Pivot_Distance_%', 'N/A')}%)\")\n",
        "                print(f\"   Volume: {rowd.get('Volume_Trend', 'N/A')} | ATR: ${rowd.get('ATR', 'N/A')}\")\n",
        "                print()\n",
        "    else:\n",
        "        print(\"   No data available (missing ibd_universe or required columns)\\n\")\n",
        "\n",
        "    print(\"‚îÅ\" * 60)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Top Swings\n",
        "    # ---------------------------\n",
        "    print(f\"\\nüìà TOP {top_n_swings} SWING SETUPS\\n\")\n",
        "\n",
        "    if not ibd_universe.empty and has_cols(ibd_universe, [\"Timeframe\", \"Tier\", \"Ticker\"]):\n",
        "        swing_candidates = ibd_universe[\n",
        "            (ibd_universe[\"Timeframe\"].isin([\"SWING\", \"POSITION\"]))\n",
        "            & (ibd_universe[\"Tier\"].isin([\"A-TIER\", \"B-TIER\"]))\n",
        "        ].copy()\n",
        "\n",
        "        swing_candidates = safe_merge_convergence(swing_candidates)\n",
        "        swing_candidates = safe_sort(swing_candidates).head(top_n_swings)\n",
        "\n",
        "        if swing_candidates.empty:\n",
        "            print(\"   No swing candidates matching filters\\n\")\n",
        "        else:\n",
        "            for i, row in enumerate(swing_candidates.itertuples(index=False), 1):\n",
        "                rowd = row._asdict()\n",
        "                rs = rowd.get(\"RS_Rating\", rowd.get(\"RS_Calculated\", \"N/A\"))\n",
        "                comp = rowd.get(\"Composite\", \"N/A\")\n",
        "\n",
        "                print(f\"{i}. {rowd.get('Ticker')} - {rowd.get('Tier')} {rowd.get('Stars', '')}\")\n",
        "                print(f\"   Sources: {rowd.get('Sources', 'IBD only')}\")\n",
        "                print(f\"   RS: {rs} | Composite: {comp}\")\n",
        "                print(f\"   Setup: {rowd.get('Pivot_Status', 'N/A')}\")\n",
        "                print()\n",
        "    else:\n",
        "        print(\"   No data available (missing ibd_universe or required columns)\\n\")\n",
        "\n",
        "    print(\"‚îÅ\" * 60)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Watch List\n",
        "    # ---------------------------\n",
        "    print(\"\\nüëÅÔ∏è  WATCH LIST (Set alerts, don't trade yet)\\n\")\n",
        "\n",
        "    if not ibd_universe.empty and has_cols(ibd_universe, [\"Timeframe\", \"Tier\", \"Ticker\"]):\n",
        "        watch_candidates = ibd_universe[\n",
        "            (ibd_universe[\"Timeframe\"] == \"WATCH\")\n",
        "            & (ibd_universe[\"Tier\"].isin([\"A-TIER\", \"B-TIER\"]))\n",
        "        ].copy()\n",
        "\n",
        "        if not watch_candidates.empty:\n",
        "            watch_list = watch_candidates[\"Ticker\"].head(10).astype(str).tolist()\n",
        "            print(f\"   {len(watch_candidates)} stocks in proper bases waiting for catalyst:\")\n",
        "            print(f\"   {', '.join(watch_list)}\")\n",
        "            if len(watch_candidates) > 10:\n",
        "                print(f\"   ...and {len(watch_candidates) - 10} more\")\n",
        "        else:\n",
        "            print(\"   No stocks currently on watch\")\n",
        "    else:\n",
        "        print(\"   No data available (missing ibd_universe or required columns)\")\n",
        "    print()\n",
        "\n",
        "    print(\"‚îÅ\" * 60)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Filtered Out\n",
        "    # ---------------------------\n",
        "    print(\"\\nüö´ FILTERED OUT TODAY\\n\")\n",
        "\n",
        "    total_tickers = len(all_sources_data) if isinstance(all_sources_data, dict) else 0\n",
        "    in_ibd = (\n",
        "        len([t for t in all_sources_data.keys() if t in ticker_lookup_db])\n",
        "        if isinstance(all_sources_data, dict) and isinstance(ticker_lookup_db, dict)\n",
        "        else 0\n",
        "    )\n",
        "    filtered = total_tickers - in_ibd\n",
        "\n",
        "    if total_tickers == 0:\n",
        "        print(\"   No source universe loaded (all_sources_data is empty/missing)\")\n",
        "    elif filtered > 0:\n",
        "        print(f\"   {filtered} tickers from various sources\")\n",
        "        print(f\"   Reasons: Not in IBD ({filtered}), Extended, or Weak setup\")\n",
        "        print(\"   Action: IGNORED\")\n",
        "    else:\n",
        "        print(\"   All tickers passed quality filter\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ Daily Brief Complete\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# Example call (requires you to pass the dependencies explicitly)\n",
        "# print(\"\\nüîÑ Generating Daily Brief...\\n\")\n",
        "# generate_daily_brief(\n",
        "#     tech_engine=tech_engine,\n",
        "#     ibd_universe=ibd_universe,\n",
        "#     convergence_df=convergence_df,\n",
        "#     all_sources_data=all_sources_data,\n",
        "#     ticker_lookup_db=ticker_lookup_db,\n",
        "# )\n"
      ],
      "metadata": {
        "id": "qXEUgfZXux99"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsUUwsFx_WRH"
      },
      "source": [
        "## üìä 10. Export & Save Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fTfbjTRt_WRH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "3d7683ca-aa9f-4b15-e53f-f4459afd932b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Export daily brief to CSV? (y/n): y\n",
            "‚úÖ Daily brief exported to daily_brief_20260203.csv\n",
            "   Total stocks: 547\n",
            "   A-TIER: 0\n",
            "   B-TIER: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2c80d112-d3e1-4356-a3e2-f5017d210a8a\", \"daily_brief_20260203.csv\", 16389)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def export_daily_brief_csv():\n",
        "    \"\"\"Export daily brief to CSV for external use\"\"\"\n",
        "\n",
        "    if ibd_universe.empty:\n",
        "        print(\"‚ùå No data to export\")\n",
        "        return\n",
        "\n",
        "    # Merge with convergence\n",
        "    export_df = ibd_universe.merge(\n",
        "        convergence_df[['Ticker', 'Stars', 'Sources', 'Convergence_Score']],\n",
        "        on='Ticker',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Sort by tier and convergence\n",
        "    export_df = export_df.sort_values(['Tier', 'Convergence_Score'], ascending=[True, False])\n",
        "\n",
        "    # Select key columns\n",
        "    key_cols = ['Ticker', 'Tier', 'Stars', 'Sources', 'Convergence_Score',\n",
        "                'Timeframe', 'Pivot_Status', 'Pivot_Distance_%', 'Volume_Trend', 'ATR']\n",
        "\n",
        "    # Add any IBD rating columns that exist\n",
        "    for col in ['Composite', 'RS_Rating', 'EPS_Rating', 'SMR', 'Acc/Dist']:\n",
        "        if col in export_df.columns:\n",
        "            key_cols.append(col)\n",
        "\n",
        "    export_subset = export_df[[col for col in key_cols if col in export_df.columns]]\n",
        "\n",
        "    # Save\n",
        "    filename = f\"daily_brief_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
        "    export_subset.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Daily brief exported to {filename}\")\n",
        "    print(f\"   Total stocks: {len(export_subset)}\")\n",
        "    print(f\"   A-TIER: {len(export_subset[export_subset['Tier'] == 'A-TIER'])}\")\n",
        "    print(f\"   B-TIER: {len(export_subset[export_subset['Tier'] == 'B-TIER'])}\")\n",
        "\n",
        "    # Download file\n",
        "    files.download(filename)\n",
        "\n",
        "# Export option\n",
        "export_choice = input(\"\\nExport daily brief to CSV? (y/n): \")\n",
        "if export_choice.lower() == 'y':\n",
        "    export_daily_brief_csv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAKt036-_WRH"
      },
      "source": [
        "## üîÑ 11. Quick Commands - Interactive Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9dCA62s_WRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f98efcd3-3e26-41e8-ff8b-13c6380d6ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üéÆ INTERACTIVE COMMAND CENTER\n",
            "============================================================\n",
            "\n",
            "Available Commands:\n",
            "  1. lookup_ticker('SYMBOL') - Quick ticker analysis\n",
            "  2. generate_daily_brief() - Regenerate full brief\n",
            "  3. View A-TIER stocks only\n",
            "  4. View multi-source convergence (‚≠ê‚≠ê‚≠ê)\n",
            "  5. Export to CSV\n",
            "\n",
            "Examples:\n",
            "  lookup_ticker('NVDA')\n",
            "  generate_daily_brief()\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéÆ INTERACTIVE COMMAND CENTER\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nAvailable Commands:\")\n",
        "print(\"  1. lookup_ticker('SYMBOL') - Quick ticker analysis\")\n",
        "print(\"  2. generate_daily_brief() - Regenerate full brief\")\n",
        "print(\"  3. View A-TIER stocks only\")\n",
        "print(\"  4. View multi-source convergence (‚≠ê‚≠ê‚≠ê)\")\n",
        "print(\"  5. Export to CSV\")\n",
        "print(\"\\nExamples:\")\n",
        "print(\"  lookup_ticker('NVDA')\")\n",
        "print(\"  generate_daily_brief()\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Quick views\n",
        "def show_a_tier():\n",
        "    \"\"\"Show all A-TIER stocks\"\"\"\n",
        "    if not ibd_universe.empty:\n",
        "        a_tier = ibd_universe[ibd_universe['Tier'] == 'A-TIER'].copy()\n",
        "\n",
        "        if not a_tier.empty:\n",
        "            # Merge with convergence\n",
        "            a_tier = a_tier.merge(\n",
        "                convergence_df[['Ticker', 'Stars', 'Convergence_Score']],\n",
        "                on='Ticker',\n",
        "                how='left'\n",
        "            )\n",
        "\n",
        "            a_tier = a_tier.sort_values('Convergence_Score', ascending=False)\n",
        "\n",
        "            print(\"\\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
        "            print(f\"‚≠ê A-TIER STOCKS ({len(a_tier)} total)\")\n",
        "            print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\")\n",
        "\n",
        "            for idx, row in a_tier.iterrows():\n",
        "                print(f\"{row['Ticker']} {row.get('Stars', '')} - {row.get('Pivot_Status', 'N/A')} | {row.get('Timeframe', 'N/A')}\")\n",
        "        else:\n",
        "            print(\"\\nNo A-TIER stocks found\")\n",
        "\n",
        "def show_high_conviction():\n",
        "    \"\"\"Show high conviction multi-source stocks\"\"\"\n",
        "    high_conv = convergence_df[convergence_df['Convergence_Score'] >= 8]\n",
        "\n",
        "    print(\"\\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
        "    print(f\"‚≠ê‚≠ê‚≠ê HIGH CONVICTION ({len(high_conv)} stocks)\")\n",
        "    print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\")\n",
        "\n",
        "    for idx, row in high_conv.iterrows():\n",
        "        ticker = row['Ticker']\n",
        "        tier = ticker_lookup_db.get(ticker, {}).get('Tier', 'Unknown')\n",
        "        print(f\"{ticker} - {tier} | Sources: {row['Sources']}\")\n",
        "\n",
        "# Interactive menu\n",
        "while True:\n",
        "    choice = input(\"\\nEnter command (or 'q' to quit): \").strip()\n",
        "\n",
        "    if choice.lower() == 'q':\n",
        "        print(\"üëã Goodbye!\")\n",
        "        break\n",
        "    elif choice == '3':\n",
        "        show_a_tier()\n",
        "    elif choice == '4':\n",
        "        show_high_conviction()\n",
        "    elif choice == '5':\n",
        "        export_daily_brief_csv()\n",
        "    elif choice.startswith('lookup_ticker'):\n",
        "        # Extract ticker from command\n",
        "        try:\n",
        "            ticker = choice.split(\"'\")[1]\n",
        "            lookup_ticker(ticker)\n",
        "        except:\n",
        "            print(\"‚ùå Invalid format. Use: lookup_ticker('SYMBOL')\")\n",
        "    elif choice == 'generate_daily_brief()' or choice == '2':\n",
        "        generate_daily_brief()\n",
        "    else:\n",
        "        print(\"‚ùå Unknown command. Try: 2, 3, 4, 5, or lookup_ticker('SYMBOL')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojk67G8G_WRI"
      },
      "source": [
        "## üìù 12. Summary & Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cXoO_9rM_WRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49532d2e-9f4b-45bc-98ae-15667e8c2f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üìã SYSTEM SUMMARY\n",
            "============================================================\n",
            "\n",
            "‚úÖ Systems Active:\n",
            "   ‚Ä¢ CANSLIM Signal Booster V6\n",
            "   ‚Ä¢ Personal CIO System\n",
            "   ‚Ä¢ Multi-source convergence scoring\n",
            "   ‚Ä¢ Instant ticker lookup\n",
            "\n",
            "üìä Data Loaded:\n",
            "   ‚Ä¢ IBD Universe: 547 stocks\n",
            "   ‚Ä¢ A-TIER: 0\n",
            "   ‚Ä¢ Multi-source tickers: 550\n",
            "   ‚Ä¢ High conviction (‚≠ê‚≠ê‚≠ê): 0\n",
            "\n",
            "üéØ Quick Access:\n",
            "   ‚Ä¢ lookup_ticker('SYMBOL') - Instant analysis\n",
            "   ‚Ä¢ generate_daily_brief() - Full pre-market brief\n",
            "   ‚Ä¢ show_a_tier() - View A-TIER stocks\n",
            "   ‚Ä¢ show_high_conviction() - Multi-source plays\n",
            "\n",
            "üìà Workflow:\n",
            "   1. Upload IBD exports daily (5-10 min)\n",
            "   2. Add tickers from other sources (2 min)\n",
            "   3. Generate daily brief (instant)\n",
            "   4. Focus on top 5 daytrades + top 3 swings\n",
            "   5. Quick lookups during market hours\n",
            "\n",
            "üí° Pro Tips:\n",
            "   ‚Ä¢ Trust A-TIER + ‚≠ê‚≠ê‚≠ê convergence most\n",
            "   ‚Ä¢ Use lookup_ticker() for unknown tickers from Primus\n",
            "   ‚Ä¢ Export CSV for external tracking/analysis\n",
            "   ‚Ä¢ Re-run brief if new data comes in\n",
            "\n",
            "============================================================\n",
            "üöÄ System Ready - Happy Trading!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìã SYSTEM SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n‚úÖ Systems Active:\")\n",
        "print(\"   ‚Ä¢ CANSLIM Signal Booster V6\")\n",
        "print(\"   ‚Ä¢ Personal CIO System\")\n",
        "print(\"   ‚Ä¢ Multi-source convergence scoring\")\n",
        "print(\"   ‚Ä¢ Instant ticker lookup\")\n",
        "\n",
        "print(\"\\nüìä Data Loaded:\")\n",
        "print(f\"   ‚Ä¢ IBD Universe: {len(ibd_universe)} stocks\")\n",
        "print(f\"   ‚Ä¢ A-TIER: {len(ibd_universe[ibd_universe['Tier'] == 'A-TIER']) if not ibd_universe.empty else 0}\")\n",
        "print(f\"   ‚Ä¢ Multi-source tickers: {len(all_sources_data)}\")\n",
        "print(f\"   ‚Ä¢ High conviction (‚≠ê‚≠ê‚≠ê): {len(convergence_df[convergence_df['Convergence_Score'] >= 8])}\")\n",
        "\n",
        "print(\"\\nüéØ Quick Access:\")\n",
        "print(\"   ‚Ä¢ lookup_ticker('SYMBOL') - Instant analysis\")\n",
        "print(\"   ‚Ä¢ generate_daily_brief() - Full pre-market brief\")\n",
        "print(\"   ‚Ä¢ show_a_tier() - View A-TIER stocks\")\n",
        "print(\"   ‚Ä¢ show_high_conviction() - Multi-source plays\")\n",
        "\n",
        "print(\"\\nüìà Workflow:\")\n",
        "print(\"   1. Upload IBD exports daily (5-10 min)\")\n",
        "print(\"   2. Add tickers from other sources (2 min)\")\n",
        "print(\"   3. Generate daily brief (instant)\")\n",
        "print(\"   4. Focus on top 5 daytrades + top 3 swings\")\n",
        "print(\"   5. Quick lookups during market hours\")\n",
        "\n",
        "print(\"\\nüí° Pro Tips:\")\n",
        "print(\"   ‚Ä¢ Trust A-TIER + ‚≠ê‚≠ê‚≠ê convergence most\")\n",
        "print(\"   ‚Ä¢ Use lookup_ticker() for unknown tickers from Primus\")\n",
        "print(\"   ‚Ä¢ Export CSV for external tracking/analysis\")\n",
        "print(\"   ‚Ä¢ Re-run brief if new data comes in\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ System Ready - Happy Trading!\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}