{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StokedDude/CANSLIM-signal-booster/blob/main/CANSLIM_Booster_V7_Standardized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BeQBCbe4Hst"
      },
      "source": [
        "# ðŸš€ CANSLIM Signal Booster V7 + Personal CIO System\n",
        "\n",
        "**Complete Trading Intelligence Platform with:**\n",
        "- âœ… Multi-source ingestion (IBD, DeepVue, Primus, Finviz, X, watchlist)\n",
        "- âœ… Convergence scoring (multi-source confirmation)\n",
        "- âœ… Dynamic trust tiers (A/B/C/D auto-classification)\n",
        "- âœ… Technical overlays (RS, volume, pivot distance, ATR)\n",
        "- âœ… Instant ticker lookup\n",
        "- âœ… Daily Brief generator (pre-market intelligence)\n",
        "- âœ… Portfolio health tracking\n",
        "- âœ… Market regime detection\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‹ Instructions\n",
        "\n",
        "1. **Run Cell 1** - Install dependencies\n",
        "2. **Run Cell 2** - Load all system code\n",
        "3. **Configure settings** in Cell 3\n",
        "4. **Run any feature** you need:\n",
        "   - Cell 4: Upload IBD Data\n",
        "   - Cell 5: Add External Sources\n",
        "   - Cell 6: Process & Score Universe\n",
        "   - Cell 7: Ticker Lookup\n",
        "   - Cell 8: Generate Daily Brief\n",
        "   - Cell 9: Export Data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7x8Ha8h4Hst",
        "outputId": "9921beac-2c47-4363-be93-f48c04b48f83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All packages installed!\n",
            "\n",
            "Installed:\n",
            "  â€¢ yfinance - Market data\n",
            "  â€¢ pandas/numpy - Data analysis\n",
            "  â€¢ scipy - Statistical functions\n",
            "  â€¢ scikit-learn - ML scoring\n",
            "  â€¢ tabulate - Table formatting\n",
            "\n",
            "ðŸ‘‰ Now run Cell 2 to load the system code.\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ“¦ **Cell 1: Install Dependencies** (Run First!)\n",
        "#@markdown Click the play button to install all required packages.\n",
        "\n",
        "!pip install -q yfinance pandas numpy scipy scikit-learn tabulate\n",
        "\n",
        "print(\"âœ… All packages installed!\")\n",
        "print(\"\")\n",
        "print(\"Installed:\")\n",
        "print(\"  â€¢ yfinance - Market data\")\n",
        "print(\"  â€¢ pandas/numpy - Data analysis\")\n",
        "print(\"  â€¢ scipy - Statistical functions\")\n",
        "print(\"  â€¢ scikit-learn - ML scoring\")\n",
        "print(\"  â€¢ tabulate - Table formatting\")\n",
        "print(\"\")\n",
        "print(\"ðŸ‘‰ Now run Cell 2 to load the system code.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e58NLSRB4Hst",
        "outputId": "fed3d83c-ed5a-4408-fd35-286a0a0c7f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All system code loaded!\n",
            "\n",
            "Components ready:\n",
            "  â€¢ TechnicalFactorEngine - RS, volume, ATR calculations\n",
            "  â€¢ DataStore - Central data management\n",
            "  â€¢ Tier Classification - A/B/C/D ranking\n",
            "  â€¢ Convergence Scoring - Multi-source confirmation\n",
            "  â€¢ Daily Brief Generator - Pre-market intelligence\n",
            "\n",
            "ðŸ‘‰ Now configure your settings in Cell 3.\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ§  **Cell 2: Load System Code** (Run Second!)\n",
        "#@markdown This cell contains all the system logic. Just run it once.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import warnings\n",
        "import io\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================\n",
        "# TECHNICAL FACTOR ENGINE\n",
        "# ============================================================\n",
        "\n",
        "class TechnicalFactorEngine:\n",
        "    \"\"\"Calculate CANSLIM technical factors with caching\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.market_data = {}\n",
        "\n",
        "    def get_price_data(self, ticker: str, period: str = \"6mo\") -> pd.DataFrame:\n",
        "        \"\"\"Fetch price data with caching\"\"\"\n",
        "        cache_key = f\"{ticker}_{period}\"\n",
        "        if cache_key not in self.market_data:\n",
        "            try:\n",
        "                data = yf.download(ticker, period=period, progress=False)\n",
        "                if not data.empty:\n",
        "                    self.market_data[cache_key] = data\n",
        "            except:\n",
        "                return pd.DataFrame()\n",
        "        return self.market_data.get(cache_key, pd.DataFrame())\n",
        "\n",
        "    def calculate_rs(self, ticker: str, weeks: int = 13) -> float:\n",
        "        \"\"\"Calculate Relative Strength vs SPY\"\"\"\n",
        "        try:\n",
        "            stock_data = self.get_price_data(ticker)\n",
        "            spy_data = self.get_price_data(\"SPY\")\n",
        "\n",
        "            if stock_data.empty or spy_data.empty:\n",
        "                return 50\n",
        "\n",
        "            days = min(weeks * 5, len(stock_data) - 1, len(spy_data) - 1)\n",
        "            if days < 5:\n",
        "                return 50\n",
        "\n",
        "            stock_return = (stock_data['Close'].iloc[-1] / stock_data['Close'].iloc[-days] - 1) * 100\n",
        "            spy_return = (spy_data['Close'].iloc[-1] / spy_data['Close'].iloc[-days] - 1) * 100\n",
        "\n",
        "            relative_perf = stock_return - spy_return\n",
        "            rs_score = min(99, max(1, 50 + (relative_perf * 2)))\n",
        "\n",
        "            return round(rs_score)\n",
        "        except:\n",
        "            return 50\n",
        "\n",
        "    def calculate_volume_trend(self, ticker: str) -> str:\n",
        "        \"\"\"Analyze volume pattern\"\"\"\n",
        "        try:\n",
        "            data = self.get_price_data(ticker)\n",
        "            if data.empty or 'Volume' not in data.columns or len(data) < 25:\n",
        "                return \"Unknown\"\n",
        "\n",
        "            recent_vol = data['Volume'].iloc[-5:].mean()\n",
        "            avg_vol = data['Volume'].iloc[-25:-5].mean()\n",
        "\n",
        "            if avg_vol == 0:\n",
        "                return \"Unknown\"\n",
        "\n",
        "            if recent_vol > avg_vol * 1.2:\n",
        "                return \"Expanding\"\n",
        "            elif recent_vol < avg_vol * 0.8:\n",
        "                return \"Contracting\"\n",
        "            else:\n",
        "                return \"Neutral\"\n",
        "        except:\n",
        "            return \"Unknown\"\n",
        "\n",
        "    def calculate_pivot_distance(self, ticker: str) -> Tuple[float, str]:\n",
        "        \"\"\"Calculate distance from 52-week high\"\"\"\n",
        "        try:\n",
        "            data = self.get_price_data(ticker, period=\"1y\")\n",
        "            if data.empty:\n",
        "                return 0, \"Unknown\"\n",
        "\n",
        "            current_price = data['Close'].iloc[-1]\n",
        "            high_52w = data['High'].max()\n",
        "\n",
        "            if high_52w == 0:\n",
        "                return 0, \"Unknown\"\n",
        "\n",
        "            distance_pct = ((current_price / high_52w) - 1) * 100\n",
        "\n",
        "            if distance_pct >= -5:\n",
        "                status = \"PROPER SETUP\"\n",
        "            elif distance_pct >= -10:\n",
        "                status = \"NEAR PIVOT\"\n",
        "            elif distance_pct >= -15:\n",
        "                status = \"PULLBACK\"\n",
        "            else:\n",
        "                status = \"EXTENDED DOWN\"\n",
        "\n",
        "            return round(distance_pct, 1), status\n",
        "        except:\n",
        "            return 0, \"Unknown\"\n",
        "\n",
        "    def calculate_atr(self, ticker: str, period: int = 14) -> float:\n",
        "        \"\"\"Calculate Average True Range\"\"\"\n",
        "        try:\n",
        "            data = self.get_price_data(ticker)\n",
        "            if data.empty or len(data) < period + 1:\n",
        "                return 0\n",
        "\n",
        "            high_low = data['High'] - data['Low']\n",
        "            high_close = abs(data['High'] - data['Close'].shift())\n",
        "            low_close = abs(data['Low'] - data['Close'].shift())\n",
        "\n",
        "            true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "            atr = true_range.rolling(window=period).mean().iloc[-1]\n",
        "\n",
        "            return round(atr, 2)\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    def determine_timeframe(self, atr: float, pivot_distance: float) -> str:\n",
        "        \"\"\"Determine trading timeframe\"\"\"\n",
        "        if atr > 5 and abs(pivot_distance) < 5:\n",
        "            return \"DAYTRADE\"\n",
        "        elif abs(pivot_distance) < 5:\n",
        "            return \"SWING\"\n",
        "        elif abs(pivot_distance) < 10:\n",
        "            return \"POSITION\"\n",
        "        else:\n",
        "            return \"WATCH\"\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DATA STORE\n",
        "# ============================================================\n",
        "\n",
        "class DataStore:\n",
        "    \"\"\"Central data storage for the system\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.ibd_universe = pd.DataFrame()\n",
        "        self.all_sources_data = defaultdict(list)\n",
        "        self.ticker_lookup_db = {}\n",
        "        self.convergence_df = pd.DataFrame()\n",
        "        self.tech_engine = TechnicalFactorEngine()\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Reset all data\"\"\"\n",
        "        self.ibd_universe = pd.DataFrame()\n",
        "        self.all_sources_data = defaultdict(list)\n",
        "        self.ticker_lookup_db = {}\n",
        "        self.convergence_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# IBD DATA LOADER\n",
        "# ============================================================\n",
        "\n",
        "def load_ibd_files(uploaded_files: Dict, data_store: DataStore) -> pd.DataFrame:\n",
        "    \"\"\"Load and merge IBD CSV exports\"\"\"\n",
        "    ibd_dataframes = []\n",
        "\n",
        "    for filename, content in uploaded_files.items():\n",
        "        try:\n",
        "            df = pd.read_csv(io.BytesIO(content))\n",
        "            df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
        "\n",
        "            ticker_col = None\n",
        "            for col in ['Symbol', 'Ticker', 'Stock', 'SYMBOL', 'TICKER']:\n",
        "                if col in df.columns:\n",
        "                    ticker_col = col\n",
        "                    break\n",
        "\n",
        "            if ticker_col:\n",
        "                df = df.rename(columns={ticker_col: 'Ticker'})\n",
        "                df['Source_File'] = filename\n",
        "                ibd_dataframes.append(df)\n",
        "                print(f\"âœ… Loaded {filename}: {len(df)} stocks\")\n",
        "            else:\n",
        "                print(f\"âš ï¸  Skipped {filename}: No ticker column found\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading {filename}: {e}\")\n",
        "\n",
        "    if ibd_dataframes:\n",
        "        data_store.ibd_universe = pd.concat(ibd_dataframes, ignore_index=True)\n",
        "        data_store.ibd_universe = data_store.ibd_universe.drop_duplicates(subset='Ticker', keep='first')\n",
        "        data_store.ibd_universe['Ticker'] = data_store.ibd_universe['Ticker'].str.strip().str.upper()\n",
        "\n",
        "        print(f\"\\n{'â”'*40}\")\n",
        "        print(f\"âœ… IBD UNIVERSE LOADED\")\n",
        "        print(f\"{'â”'*40}\")\n",
        "        print(f\"Total unique stocks: {len(data_store.ibd_universe)}\")\n",
        "        print(f\"Files processed: {len(ibd_dataframes)}\")\n",
        "        print(f\"\\nSample: {', '.join(data_store.ibd_universe['Ticker'].head(10).tolist())}\")\n",
        "\n",
        "    return data_store.ibd_universe\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# EXTERNAL SOURCE HANDLER\n",
        "# ============================================================\n",
        "\n",
        "def add_source_tickers(source_name: str, tickers_input: str, data_store: DataStore) -> List[str]:\n",
        "    \"\"\"Add tickers from an external source\"\"\"\n",
        "    tickers = [t.strip().upper() for t in tickers_input.replace(',', ' ').split() if t.strip()]\n",
        "\n",
        "    for ticker in tickers:\n",
        "        if source_name not in data_store.all_sources_data[ticker]:\n",
        "            data_store.all_sources_data[ticker].append(source_name)\n",
        "\n",
        "    return tickers\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TECHNICAL OVERLAY PROCESSOR\n",
        "# ============================================================\n",
        "\n",
        "def process_technical_overlay(data_store: DataStore, max_workers: int = 12) -> pd.DataFrame:\n",
        "    \"\"\"Add technical factors to IBD universe\"\"\"\n",
        "\n",
        "    if data_store.ibd_universe.empty:\n",
        "        print(\"âŒ No IBD universe to process\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(\"ðŸ”„ Processing technical factors...\")\n",
        "    print(f\"Analyzing {len(data_store.ibd_universe)} stocks\\n\")\n",
        "\n",
        "    # Warm up cache\n",
        "    _ = data_store.tech_engine.get_price_data(\"SPY\")\n",
        "\n",
        "    tickers = data_store.ibd_universe['Ticker'].tolist()\n",
        "    total = len(tickers)\n",
        "\n",
        "    def compute_one(ticker: str) -> Dict[str, Any]:\n",
        "        try:\n",
        "            rs = data_store.tech_engine.calculate_rs(ticker)\n",
        "            volume = data_store.tech_engine.calculate_volume_trend(ticker)\n",
        "            pivot_dist, pivot_status = data_store.tech_engine.calculate_pivot_distance(ticker)\n",
        "            atr = data_store.tech_engine.calculate_atr(ticker)\n",
        "            timeframe = data_store.tech_engine.determine_timeframe(atr, pivot_dist)\n",
        "\n",
        "            return {\n",
        "                'Ticker': ticker,\n",
        "                'RS_Calculated': rs,\n",
        "                'Volume_Trend': volume,\n",
        "                'Pivot_Distance_%': pivot_dist,\n",
        "                'Pivot_Status': pivot_status,\n",
        "                'ATR': atr,\n",
        "                'Timeframe': timeframe,\n",
        "                'Tech_Status': 'OK'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'Ticker': ticker,\n",
        "                'RS_Calculated': None,\n",
        "                'Volume_Trend': None,\n",
        "                'Pivot_Distance_%': None,\n",
        "                'Pivot_Status': None,\n",
        "                'ATR': None,\n",
        "                'Timeframe': None,\n",
        "                'Tech_Status': f'FAIL: {e}'\n",
        "            }\n",
        "\n",
        "    results = []\n",
        "    chunk_size = 50\n",
        "\n",
        "    for start in range(0, total, chunk_size):\n",
        "        chunk = tickers[start:start + chunk_size]\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            futures = {executor.submit(compute_one, t): t for t in chunk}\n",
        "            for fut in as_completed(futures):\n",
        "                results.append(fut.result())\n",
        "\n",
        "        processed = min(start + chunk_size, total)\n",
        "        print(f\"Progress: {processed}/{total} ({(processed/total)*100:.0f}%)\")\n",
        "\n",
        "    tech_df = pd.DataFrame(results)\n",
        "    data_store.ibd_universe = data_store.ibd_universe.merge(tech_df, on='Ticker', how='left')\n",
        "\n",
        "    print(f\"\\nâœ… Technical overlay complete!\")\n",
        "    return data_store.ibd_universe\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TIER CLASSIFICATION\n",
        "# ============================================================\n",
        "\n",
        "def classify_tier(row: pd.Series) -> str:\n",
        "    \"\"\"Assign A/B/C/D tier based on CANSLIM criteria\"\"\"\n",
        "\n",
        "    rs = None\n",
        "    for col in ['RS_Rating', 'RS', 'Relative_Strength', 'RS_Calculated']:\n",
        "        if col in row.index and pd.notna(row[col]):\n",
        "            rs = row[col]\n",
        "            break\n",
        "    rs = rs if rs is not None else 50\n",
        "\n",
        "    composite = None\n",
        "    for col in ['Composite', 'Composite_Rating', 'Comp', 'Comp_Rating']:\n",
        "        if col in row.index and pd.notna(row[col]):\n",
        "            composite = row[col]\n",
        "            break\n",
        "    composite = composite if composite is not None else 50\n",
        "\n",
        "    pivot_status = row.get('Pivot_Status', 'Unknown')\n",
        "\n",
        "    if rs >= 90 and composite >= 90 and pivot_status == \"PROPER SETUP\":\n",
        "        return \"A-TIER\"\n",
        "    elif rs >= 80 and composite >= 80:\n",
        "        return \"B-TIER\"\n",
        "    elif rs >= 70 or composite >= 70:\n",
        "        return \"C-TIER\"\n",
        "    else:\n",
        "        return \"D-TIER\"\n",
        "\n",
        "\n",
        "def apply_tier_classification(data_store: DataStore) -> pd.DataFrame:\n",
        "    \"\"\"Apply tier classification to universe\"\"\"\n",
        "\n",
        "    if data_store.ibd_universe.empty:\n",
        "        print(\"âŒ No data to classify\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    data_store.ibd_universe['Tier'] = data_store.ibd_universe.apply(classify_tier, axis=1)\n",
        "    data_store.ibd_universe['Verification'] = 'IBD-VERIFIED'\n",
        "\n",
        "    # Build lookup database\n",
        "    for _, row in data_store.ibd_universe.iterrows():\n",
        "        data_store.ticker_lookup_db[row['Ticker']] = row.to_dict()\n",
        "\n",
        "    # Add IBD tickers to sources\n",
        "    for _, row in data_store.ibd_universe.iterrows():\n",
        "        ticker = row['Ticker']\n",
        "        rs = row.get('RS_Rating', row.get('RS_Calculated', 50))\n",
        "        source = 'IBD_50' if rs and rs >= 90 else 'IBD_250'\n",
        "        if source not in data_store.all_sources_data[ticker]:\n",
        "            data_store.all_sources_data[ticker].append(source)\n",
        "\n",
        "    tier_counts = data_store.ibd_universe['Tier'].value_counts()\n",
        "\n",
        "    print(f\"{'â”'*40}\")\n",
        "    print(\"ðŸ“Š TIER CLASSIFICATION\")\n",
        "    print(f\"{'â”'*40}\")\n",
        "    for tier in ['A-TIER', 'B-TIER', 'C-TIER', 'D-TIER']:\n",
        "        print(f\"{tier}: {tier_counts.get(tier, 0)} stocks\")\n",
        "    print(f\"\\nâœ… Classification complete!\")\n",
        "    print(f\"âœ… Lookup database: {len(data_store.ticker_lookup_db)} stocks\")\n",
        "\n",
        "    return data_store.ibd_universe\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# CONVERGENCE SCORING\n",
        "# ============================================================\n",
        "\n",
        "SOURCE_WEIGHTS = {\n",
        "    'IBD_50': 3,\n",
        "    'IBD_250': 2,\n",
        "    'DeepVue': 2,\n",
        "    'Primus': 2,\n",
        "    'Finviz': 1,\n",
        "    'X_Mentions': 1,\n",
        "    'Watchlist': 1\n",
        "}\n",
        "\n",
        "\n",
        "def calculate_convergence(data_store: DataStore) -> pd.DataFrame:\n",
        "    \"\"\"Calculate convergence scores for all tickers\"\"\"\n",
        "\n",
        "    convergence_data = []\n",
        "    max_weight = sum(SOURCE_WEIGHTS.values())\n",
        "\n",
        "    for ticker, sources in data_store.all_sources_data.items():\n",
        "        total_weight = sum(SOURCE_WEIGHTS.get(s, 1) for s in sources)\n",
        "        score = min(10, int((total_weight / max_weight) * 10))\n",
        "\n",
        "        if score >= 8:\n",
        "            stars = \"â­â­â­\"\n",
        "        elif score >= 6:\n",
        "            stars = \"â­â­\"\n",
        "        elif score >= 4:\n",
        "            stars = \"â­\"\n",
        "        else:\n",
        "            stars = \"\"\n",
        "\n",
        "        convergence_data.append({\n",
        "            'Ticker': ticker,\n",
        "            'Sources': ', '.join(sources),\n",
        "            'Source_Count': len(sources),\n",
        "            'Convergence_Score': score,\n",
        "            'Stars': stars\n",
        "        })\n",
        "\n",
        "    data_store.convergence_df = pd.DataFrame(convergence_data).sort_values(\n",
        "        'Convergence_Score', ascending=False\n",
        "    )\n",
        "\n",
        "    high = len(data_store.convergence_df[data_store.convergence_df['Convergence_Score'] >= 8])\n",
        "    med = len(data_store.convergence_df[data_store.convergence_df['Convergence_Score'].between(6, 7)])\n",
        "\n",
        "    print(f\"{'â”'*40}\")\n",
        "    print(\"â­ CONVERGENCE SCORING\")\n",
        "    print(f\"{'â”'*40}\")\n",
        "    print(f\"Total tickers: {len(data_store.convergence_df)}\")\n",
        "    print(f\"High conviction (â­â­â­): {high}\")\n",
        "    print(f\"Medium conviction (â­â­): {med}\")\n",
        "    print(f\"\\nâœ… Convergence analysis complete!\")\n",
        "\n",
        "    return data_store.convergence_df\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TICKER LOOKUP\n",
        "# ============================================================\n",
        "\n",
        "def lookup_ticker(ticker: str, data_store: DataStore) -> None:\n",
        "    \"\"\"Instant ticker quality lookup\"\"\"\n",
        "\n",
        "    ticker = ticker.strip().upper()\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"ðŸ“Š TICKER LOOKUP: {ticker}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    conv_row = data_store.convergence_df[\n",
        "        data_store.convergence_df['Ticker'] == ticker\n",
        "    ] if not data_store.convergence_df.empty else pd.DataFrame()\n",
        "\n",
        "    if ticker in data_store.ticker_lookup_db:\n",
        "        data = data_store.ticker_lookup_db[ticker]\n",
        "\n",
        "        sources = conv_row['Sources'].iloc[0] if not conv_row.empty else 'IBD only'\n",
        "        stars = conv_row['Stars'].iloc[0] if not conv_row.empty else ''\n",
        "\n",
        "        tier = data.get('Tier', 'UNKNOWN')\n",
        "        print(f\"\\nâœ… {tier} - IBD VERIFIED\")\n",
        "        print(f\"\\nConviction: {stars}\")\n",
        "        print(f\"Sources: {sources}\")\n",
        "\n",
        "        print(\"\\nðŸ“ˆ CANSLIM Ratings:\")\n",
        "        for col in ['Composite', 'Comp_Rating', 'RS_Rating', 'EPS_Rating', 'SMR_Rating', 'A/D_Rating']:\n",
        "            if col in data and pd.notna(data[col]):\n",
        "                print(f\"  {col}: {data[col]}\")\n",
        "\n",
        "        print(\"\\nðŸ“Š Technical Analysis:\")\n",
        "        print(f\"  Pivot Status: {data.get('Pivot_Status', 'N/A')}\")\n",
        "        print(f\"  Distance from Pivot: {data.get('Pivot_Distance_%', 'N/A')}%\")\n",
        "        print(f\"  Volume Trend: {data.get('Volume_Trend', 'N/A')}\")\n",
        "        print(f\"  ATR: ${data.get('ATR', 'N/A')}\")\n",
        "        print(f\"  Timeframe: {data.get('Timeframe', 'N/A')}\")\n",
        "\n",
        "        pivot_status = str(data.get('Pivot_Status', '')).upper()\n",
        "\n",
        "        print(\"\\nðŸ’¡ Recommendation:\")\n",
        "        if tier == 'A-TIER' and pivot_status == 'PROPER SETUP':\n",
        "            print(\"  ðŸŸ¢ HIGH CONVICTION - Ready to trade\")\n",
        "        elif tier == 'A-TIER':\n",
        "            print(\"  ðŸŸ¡ QUALITY STOCK - Wait for better entry\")\n",
        "        elif tier == 'B-TIER' and pivot_status == 'PROPER SETUP':\n",
        "            print(\"  ðŸŸ¡ WATCHABLE - Consider smaller position\")\n",
        "        elif tier == 'B-TIER':\n",
        "            print(\"  ðŸŸ¡ WATCH - Set alerts for entry\")\n",
        "        else:\n",
        "            print(\"  ðŸ”´ PASS - Low conviction\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nâš ï¸  NOT IN IBD DATABASE\")\n",
        "\n",
        "        if not conv_row.empty:\n",
        "            sources = conv_row['Sources'].iloc[0]\n",
        "            print(f\"\\nFound in: {sources}\")\n",
        "            print(\"\\nCalculating technical scores...\")\n",
        "\n",
        "            try:\n",
        "                rs = data_store.tech_engine.calculate_rs(ticker)\n",
        "                volume = data_store.tech_engine.calculate_volume_trend(ticker)\n",
        "                pivot_dist, pivot_status = data_store.tech_engine.calculate_pivot_distance(ticker)\n",
        "                atr = data_store.tech_engine.calculate_atr(ticker)\n",
        "\n",
        "                print(\"\\nðŸ“Š TECHNICAL-ONLY Analysis:\")\n",
        "                print(f\"  RS (estimated): {rs}\")\n",
        "                print(f\"  Volume: {volume}\")\n",
        "                print(f\"  Pivot Status: {pivot_status} ({pivot_dist}%)\")\n",
        "                print(f\"  ATR: ${atr}\")\n",
        "\n",
        "                if rs >= 80 and pivot_status == 'PROPER SETUP':\n",
        "                    grade = \"B-TIER (Technical-only)\"\n",
        "                    rec = \"ðŸŸ¡ WATCHABLE - No fundamental verification\"\n",
        "                elif rs >= 70:\n",
        "                    grade = \"C-TIER (Technical-only)\"\n",
        "                    rec = \"ðŸŸ  DAYTRADE ONLY - Tight stops\"\n",
        "                else:\n",
        "                    grade = \"D-TIER\"\n",
        "                    rec = \"ðŸ”´ PASS - Weak metrics\"\n",
        "\n",
        "                print(f\"\\nGrade: {grade}\")\n",
        "                print(f\"\\nðŸ’¡ Recommendation: {rec}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nâŒ Technical calc failed: {e}\")\n",
        "        else:\n",
        "            print(\"\\nâŒ Not found in any source\")\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DAILY BRIEF GENERATOR\n",
        "# ============================================================\n",
        "\n",
        "def generate_daily_brief(data_store: DataStore, top_daytrades: int = 5, top_swings: int = 3) -> None:\n",
        "    \"\"\"Generate comprehensive pre-market daily brief\"\"\"\n",
        "\n",
        "    current_date = datetime.now().strftime('%A, %B %d, %Y')\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ðŸ“‹ DAILY TRADING BRIEF\")\n",
        "    print(f\"{current_date} | Pre-Market\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Market Regime\n",
        "    try:\n",
        "        spy_data = data_store.tech_engine.get_price_data('SPY', period='1mo')\n",
        "        if len(spy_data) >= 6:\n",
        "            spy_return = ((spy_data['Close'].iloc[-1] / spy_data['Close'].iloc[-5]) - 1) * 100\n",
        "\n",
        "            vix_data = data_store.tech_engine.get_price_data('^VIX', period='1mo')\n",
        "            vix = float(vix_data['Close'].iloc[-1]) if not vix_data.empty else 15.0\n",
        "\n",
        "            if spy_return > 1 and vix < 15:\n",
        "                regime = \"Trending Up âœ…\"\n",
        "                sizing = \"Full position sizing OK\"\n",
        "            elif spy_return < -1 or vix > 20:\n",
        "                regime = \"Choppy / Distribution âš ï¸\"\n",
        "                sizing = \"Reduce size 50%\"\n",
        "            else:\n",
        "                regime = \"Neutral\"\n",
        "                sizing = \"Normal sizing\"\n",
        "\n",
        "            print(f\"\\nMARKET REGIME: {regime}\")\n",
        "            print(f\"Recommendation: {sizing}\")\n",
        "            print(f\"SPY (5-day): {spy_return:+.1f}% | VIX: {vix:.1f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nMARKET REGIME: Unable to fetch ({e})\")\n",
        "\n",
        "    print(f\"\\n{'â”'*60}\")\n",
        "\n",
        "    # Merge convergence data\n",
        "    if not data_store.ibd_universe.empty and not data_store.convergence_df.empty:\n",
        "        merged = data_store.ibd_universe.merge(\n",
        "            data_store.convergence_df[['Ticker', 'Stars', 'Sources', 'Convergence_Score']],\n",
        "            on='Ticker', how='left'\n",
        "        )\n",
        "    else:\n",
        "        merged = data_store.ibd_universe.copy()\n",
        "\n",
        "    # Top Daytrades\n",
        "    print(f\"\\nðŸŽ¯ TOP {top_daytrades} DAYTRADE SETUPS\\n\")\n",
        "\n",
        "    if 'Timeframe' in merged.columns and 'Tier' in merged.columns:\n",
        "        daytrades = merged[\n",
        "            (merged['Timeframe'] == 'DAYTRADE') &\n",
        "            (merged['Tier'].isin(['A-TIER', 'B-TIER']))\n",
        "        ].head(top_daytrades)\n",
        "\n",
        "        if daytrades.empty:\n",
        "            print(\"   No daytrade setups matching criteria\")\n",
        "        else:\n",
        "            for i, row in daytrades.iterrows():\n",
        "                stars = row.get('Stars', '')\n",
        "                print(f\"{row['Ticker']} - {row['Tier']} {stars}\")\n",
        "                print(f\"   Setup: {row.get('Pivot_Status', 'N/A')} | ATR: ${row.get('ATR', 'N/A')}\")\n",
        "    else:\n",
        "        print(\"   No data available\")\n",
        "\n",
        "    print(f\"\\n{'â”'*60}\")\n",
        "\n",
        "    # Top Swings\n",
        "    print(f\"\\nðŸ“ˆ TOP {top_swings} SWING SETUPS\\n\")\n",
        "\n",
        "    if 'Timeframe' in merged.columns and 'Tier' in merged.columns:\n",
        "        swings = merged[\n",
        "            (merged['Timeframe'].isin(['SWING', 'POSITION'])) &\n",
        "            (merged['Tier'].isin(['A-TIER', 'B-TIER']))\n",
        "        ].head(top_swings)\n",
        "\n",
        "        if swings.empty:\n",
        "            print(\"   No swing setups matching criteria\")\n",
        "        else:\n",
        "            for i, row in swings.iterrows():\n",
        "                stars = row.get('Stars', '')\n",
        "                print(f\"{row['Ticker']} - {row['Tier']} {stars}\")\n",
        "                print(f\"   Setup: {row.get('Pivot_Status', 'N/A')}\")\n",
        "    else:\n",
        "        print(\"   No data available\")\n",
        "\n",
        "    print(f\"\\n{'â”'*60}\")\n",
        "\n",
        "    # Watch List\n",
        "    print(\"\\nðŸ‘ï¸  WATCH LIST\\n\")\n",
        "\n",
        "    if 'Timeframe' in merged.columns:\n",
        "        watch = merged[\n",
        "            (merged['Timeframe'] == 'WATCH') &\n",
        "            (merged['Tier'].isin(['A-TIER', 'B-TIER']))\n",
        "        ]\n",
        "        if not watch.empty:\n",
        "            tickers = watch['Ticker'].head(10).tolist()\n",
        "            print(f\"   {', '.join(tickers)}\")\n",
        "            if len(watch) > 10:\n",
        "                print(f\"   ...and {len(watch) - 10} more\")\n",
        "        else:\n",
        "            print(\"   No stocks on watch\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"âœ… Daily Brief Complete\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# EXPORT FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "def export_to_csv(data_store: DataStore, filename: str = None) -> str:\n",
        "    \"\"\"Export daily brief to CSV\"\"\"\n",
        "\n",
        "    if data_store.ibd_universe.empty:\n",
        "        print(\"âŒ No data to export\")\n",
        "        return None\n",
        "\n",
        "    if filename is None:\n",
        "        filename = f\"canslim_brief_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
        "\n",
        "    # Merge with convergence\n",
        "    if not data_store.convergence_df.empty:\n",
        "        export_df = data_store.ibd_universe.merge(\n",
        "            data_store.convergence_df[['Ticker', 'Stars', 'Sources', 'Convergence_Score']],\n",
        "            on='Ticker', how='left'\n",
        "        )\n",
        "    else:\n",
        "        export_df = data_store.ibd_universe.copy()\n",
        "\n",
        "    export_df = export_df.sort_values(['Tier', 'Convergence_Score'] if 'Convergence_Score' in export_df.columns else ['Tier'],\n",
        "                                       ascending=[True, False] if 'Convergence_Score' in export_df.columns else [True])\n",
        "\n",
        "    export_df.to_csv(filename, index=False)\n",
        "\n",
        "    tier_counts = export_df['Tier'].value_counts() if 'Tier' in export_df.columns else {}\n",
        "\n",
        "    print(f\"âœ… Exported to {filename}\")\n",
        "    print(f\"   Total: {len(export_df)} stocks\")\n",
        "    print(f\"   A-TIER: {tier_counts.get('A-TIER', 0)}\")\n",
        "    print(f\"   B-TIER: {tier_counts.get('B-TIER', 0)}\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# INITIALIZE SYSTEM\n",
        "# ============================================================\n",
        "\n",
        "# Create global data store\n",
        "DATA = DataStore()\n",
        "\n",
        "print(\"âœ… All system code loaded!\")\n",
        "print(\"\")\n",
        "print(\"Components ready:\")\n",
        "print(\"  â€¢ TechnicalFactorEngine - RS, volume, ATR calculations\")\n",
        "print(\"  â€¢ DataStore - Central data management\")\n",
        "print(\"  â€¢ Tier Classification - A/B/C/D ranking\")\n",
        "print(\"  â€¢ Convergence Scoring - Multi-source confirmation\")\n",
        "print(\"  â€¢ Daily Brief Generator - Pre-market intelligence\")\n",
        "print(\"\")\n",
        "print(\"ðŸ‘‰ Now configure your settings in Cell 3.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ-J3Yk_4Hsu",
        "outputId": "4048eadf-8d23-4076-e275-a2467632d7bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Configuration saved!\n",
            "\n",
            "Settings:\n",
            "  Max Workers: 12\n",
            "  Top Daytrades: 5\n",
            "  Top Swings: 5\n",
            "  \n",
            "Tier Thresholds:\n",
            "  A-TIER: RS >= 90\n",
            "  B-TIER: RS >= 80\n",
            "  C-TIER: RS >= 70\n",
            "  \n",
            "Source Weights:\n",
            "  IBD 50: 3 | IBD 250: 2\n",
            "  DeepVue: 2 | Primus: 2\n",
            "  Finviz: 2 | X: 2 | Watchlist: 5\n",
            "\n",
            "ðŸ‘‰ Now upload your IBD data in Cell 4.\n"
          ]
        }
      ],
      "source": [
        "#@title âš™ï¸ **Cell 3: Configuration**\n",
        "#@markdown ### Processing Settings\n",
        "MAX_WORKERS = 12 #@param {type:\"slider\", min:4, max:24, step:4}\n",
        "\n",
        "#@markdown ### Daily Brief Settings\n",
        "TOP_DAYTRADES = 5 #@param {type:\"slider\", min:3, max:10, step:1}\n",
        "TOP_SWINGS = 5 #@param {type:\"slider\", min:2, max:8, step:1}\n",
        "\n",
        "#@markdown ### Tier Thresholds\n",
        "A_TIER_RS_MIN = 90 #@param {type:\"slider\", min:80, max:99, step:5}\n",
        "B_TIER_RS_MIN = 80 #@param {type:\"slider\", min:70, max:90, step:5}\n",
        "C_TIER_RS_MIN = 70 #@param {type:\"slider\", min:60, max:80, step:5}\n",
        "\n",
        "#@markdown ### Source Weights (for convergence scoring)\n",
        "WEIGHT_IBD_50 = 3 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "WEIGHT_IBD_250 = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "WEIGHT_DEEPVUE = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "WEIGHT_PRIMUS = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "WEIGHT_FINVIZ = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "WEIGHT_X = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "WEIGHT_WATCHLIST = 5 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "\n",
        "# Update source weights\n",
        "SOURCE_WEIGHTS.update({\n",
        "    'IBD_50': WEIGHT_IBD_50,\n",
        "    'IBD_250': WEIGHT_IBD_250,\n",
        "    'DeepVue': WEIGHT_DEEPVUE,\n",
        "    'Primus': WEIGHT_PRIMUS,\n",
        "    'Finviz': WEIGHT_FINVIZ,\n",
        "    'X_Mentions': WEIGHT_X,\n",
        "    'Watchlist': WEIGHT_WATCHLIST\n",
        "})\n",
        "\n",
        "# Store config\n",
        "CONFIG = {\n",
        "    'max_workers': MAX_WORKERS,\n",
        "    'top_daytrades': TOP_DAYTRADES,\n",
        "    'top_swings': TOP_SWINGS,\n",
        "    'a_tier_rs': A_TIER_RS_MIN,\n",
        "    'b_tier_rs': B_TIER_RS_MIN,\n",
        "    'c_tier_rs': C_TIER_RS_MIN,\n",
        "}\n",
        "\n",
        "print(\"âœ… Configuration saved!\")\n",
        "print(f\"\"\"\n",
        "Settings:\n",
        "  Max Workers: {MAX_WORKERS}\n",
        "  Top Daytrades: {TOP_DAYTRADES}\n",
        "  Top Swings: {TOP_SWINGS}\n",
        "\n",
        "Tier Thresholds:\n",
        "  A-TIER: RS >= {A_TIER_RS_MIN}\n",
        "  B-TIER: RS >= {B_TIER_RS_MIN}\n",
        "  C-TIER: RS >= {C_TIER_RS_MIN}\n",
        "\n",
        "Source Weights:\n",
        "  IBD 50: {WEIGHT_IBD_50} | IBD 250: {WEIGHT_IBD_250}\n",
        "  DeepVue: {WEIGHT_DEEPVUE} | Primus: {WEIGHT_PRIMUS}\n",
        "  Finviz: {WEIGHT_FINVIZ} | X: {WEIGHT_X} | Watchlist: {WEIGHT_WATCHLIST}\n",
        "\"\"\")\n",
        "print(\"ðŸ‘‰ Now upload your IBD data in Cell 4.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "iJhFneQN4Hsu",
        "outputId": "ec64c887-d62c-4062-ae68-30299f8080af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¤ Upload your IBD CSV files\n",
            "You can select multiple files at once...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5e96e01a-4f38-4457-bb96-8ddc4d18008c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5e96e01a-4f38-4457-bb96-8ddc4d18008c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Deepvue Leaders.csv to Deepvue Leaders (1).csv\n",
            "Saving IBD 50 Index.csv to IBD 50 Index (1).csv\n",
            "Saving IBD Big Cap 20.csv to IBD Big Cap 20 (1).csv\n",
            "Saving MarketSurge Growth 250.csv to MarketSurge Growth 250 (1).csv\n",
            "âœ… Loaded Deepvue Leaders (1).csv: 291 stocks\n",
            "âœ… Loaded IBD 50 Index (1).csv: 50 stocks\n",
            "âœ… Loaded IBD Big Cap 20 (1).csv: 20 stocks\n",
            "âœ… Loaded MarketSurge Growth 250 (1).csv: 299 stocks\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "âœ… IBD UNIVERSE LOADED\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "Total unique stocks: 498\n",
            "Files processed: 4\n",
            "\n",
            "Sample: CALY, SNDK, EQPT, IBRX, ENPH, SLAB, AAP, AMN, VSNT, BE\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ“‚ **Cell 4: Upload IBD Data**\n",
        "#@markdown Upload your IBD CSV exports (IBD 50, IBD 250, Sector Leaders, etc.)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"ðŸ“¤ Upload your IBD CSV files\")\n",
        "print(\"You can select multiple files at once...\\n\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    load_ibd_files(uploaded, DATA)\n",
        "else:\n",
        "    print(\"âŒ No files uploaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrDOAuia4Hsv",
        "outputId": "5536cca7-acfe-41ef-b378-247d10dd0e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "ðŸ“¥ ADDING EXTERNAL SOURCES\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "\n",
            "â„¹ï¸  No external sources added (all fields empty)\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ“¥ **Cell 5: Add External Sources**\n",
        "#@markdown Enter tickers from other sources (comma or space separated)\n",
        "\n",
        "DEEPVUE_TICKERS = \"\" #@param {type:\"string\"}\n",
        "PRIMUS_TICKERS = \"\" #@param {type:\"string\"}\n",
        "FINVIZ_TICKERS = \"\" #@param {type:\"string\"}\n",
        "X_MENTIONS = \"\" #@param {type:\"string\"}\n",
        "WATCHLIST = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"â”\" * 40)\n",
        "print(\"ðŸ“¥ ADDING EXTERNAL SOURCES\")\n",
        "print(\"â”\" * 40 + \"\\n\")\n",
        "\n",
        "sources_added = 0\n",
        "\n",
        "if DEEPVUE_TICKERS.strip():\n",
        "    tickers = add_source_tickers('DeepVue', DEEPVUE_TICKERS, DATA)\n",
        "    print(f\"âœ… DeepVue: {len(tickers)} tickers added\")\n",
        "    sources_added += len(tickers)\n",
        "\n",
        "if PRIMUS_TICKERS.strip():\n",
        "    tickers = add_source_tickers('Primus', PRIMUS_TICKERS, DATA)\n",
        "    print(f\"âœ… Primus: {len(tickers)} tickers added\")\n",
        "    sources_added += len(tickers)\n",
        "\n",
        "if FINVIZ_TICKERS.strip():\n",
        "    tickers = add_source_tickers('Finviz', FINVIZ_TICKERS, DATA)\n",
        "    print(f\"âœ… Finviz: {len(tickers)} tickers added\")\n",
        "    sources_added += len(tickers)\n",
        "\n",
        "if X_MENTIONS.strip():\n",
        "    tickers = add_source_tickers('X_Mentions', X_MENTIONS, DATA)\n",
        "    print(f\"âœ… X/Twitter: {len(tickers)} tickers added\")\n",
        "    sources_added += len(tickers)\n",
        "\n",
        "if WATCHLIST.strip():\n",
        "    tickers = add_source_tickers('Watchlist', WATCHLIST, DATA)\n",
        "    print(f\"âœ… Watchlist: {len(tickers)} tickers added\")\n",
        "    sources_added += len(tickers)\n",
        "\n",
        "if sources_added == 0:\n",
        "    print(\"â„¹ï¸  No external sources added (all fields empty)\")\n",
        "else:\n",
        "    print(f\"\\nâœ… Total unique tickers from all sources: {len(DATA.all_sources_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fPImra04Hsv",
        "outputId": "6571a142-047d-48d0-f0b6-a1e6283883d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Processing universe...\n",
            "\n",
            "ðŸ”„ Processing technical factors...\n",
            "Analyzing 498 stocks\n",
            "\n",
            "Progress: 50/498 (10%)\n",
            "Progress: 100/498 (20%)\n",
            "Progress: 150/498 (30%)\n",
            "Progress: 200/498 (40%)\n",
            "Progress: 250/498 (50%)\n",
            "Progress: 300/498 (60%)\n",
            "Progress: 350/498 (70%)\n",
            "Progress: 400/498 (80%)\n",
            "Progress: 450/498 (90%)\n",
            "Progress: 498/498 (100%)\n",
            "\n",
            "âœ… Technical overlay complete!\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "ðŸ“Š TIER CLASSIFICATION\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "A-TIER: 0 stocks\n",
            "B-TIER: 160 stocks\n",
            "C-TIER: 46 stocks\n",
            "D-TIER: 292 stocks\n",
            "\n",
            "âœ… Classification complete!\n",
            "âœ… Lookup database: 498 stocks\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "â­ CONVERGENCE SCORING\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "Total tickers: 498\n",
            "High conviction (â­â­â­): 0\n",
            "Medium conviction (â­â­): 0\n",
            "\n",
            "âœ… Convergence analysis complete!\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "âœ… PROCESSING COMPLETE\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "Universe: 498 stocks\n",
            "Lookup DB: 498 entries\n",
            "\n",
            "ðŸ‘‰ Use Cell 7 for ticker lookup or Cell 8 for daily brief.\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ”„ **Cell 6: Process & Score Universe**\n",
        "#@markdown Run technical analysis, tier classification, and convergence scoring.\n",
        "\n",
        "RUN_TECHNICAL_OVERLAY = True #@param {type:\"boolean\"}\n",
        "\n",
        "if 'CONFIG' not in globals():\n",
        "    print(\"âŒ Error: Configuration (Cell 3) has not been loaded. Please run Cell 3 first.\")\n",
        "    print(\"Skipping processing until configuration is defined.\")\n",
        "elif DATA.ibd_universe.empty:\n",
        "    print(\"âŒ No IBD data loaded. Please run Cell 4 first.\")\n",
        "else:\n",
        "    print(\"ðŸ”„ Processing universe...\\n\")\n",
        "\n",
        "    # Step 1: Technical overlay (optional - takes time)\n",
        "    if RUN_TECHNICAL_OVERLAY:\n",
        "        process_technical_overlay(DATA, max_workers=CONFIG['max_workers'])\n",
        "        print(\"\")\n",
        "    else:\n",
        "        print(\"â­ï¸  Skipping technical overlay\")\n",
        "        print(\"   (Enable RUN_TECHNICAL_OVERLAY for full analysis)\\n\")\n",
        "\n",
        "    # Ensure relevant rating columns are numeric before classification\n",
        "    rating_cols_to_convert = [\n",
        "        'RS_Rating', 'RS', 'Relative_Strength',\n",
        "        'Composite', 'Composite_Rating', 'Comp', 'Comp_Rating'\n",
        "    ]\n",
        "    for col in rating_cols_to_convert:\n",
        "        if col in DATA.ibd_universe.columns:\n",
        "            DATA.ibd_universe[col] = pd.to_numeric(DATA.ibd_universe[col], errors='coerce')\n",
        "\n",
        "    # Step 2: Tier classification\n",
        "    apply_tier_classification(DATA)\n",
        "    print(\"\")\n",
        "\n",
        "    # Step 3: Convergence scoring\n",
        "    calculate_convergence(DATA)\n",
        "\n",
        "    print(\"\\n\" + \"â”\" * 40)\n",
        "    print(\"âœ… PROCESSING COMPLETE\")\n",
        "    print(\"â”\" * 40)\n",
        "    print(f\"Universe: {len(DATA.ibd_universe)} stocks\")\n",
        "    print(f\"Lookup DB: {len(DATA.ticker_lookup_db)} entries\")\n",
        "    print(f\"\\nðŸ‘‰ Use Cell 7 for ticker lookup or Cell 8 for daily brief.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk9oF0d_4Hsv",
        "outputId": "624854b8-c4c6-4a1d-ad40-28257ee016a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸ“Š TICKER LOOKUP: GOOGL\n",
            "==================================================\n",
            "\n",
            "âœ… B-TIER - IBD VERIFIED\n",
            "\n",
            "Conviction: \n",
            "Sources: IBD_50\n",
            "\n",
            "ðŸ“ˆ CANSLIM Ratings:\n",
            "  Comp_Rating: 98.0\n",
            "  RS_Rating: 93.0\n",
            "  EPS_Rating: 92.0\n",
            "  SMR_Rating: A\n",
            "  A/D_Rating: A-\n",
            "\n",
            "ðŸ“Š Technical Analysis:\n",
            "  Pivot Status: Unknown\n",
            "  Distance from Pivot: 0%\n",
            "  Volume Trend: Unknown\n",
            "  ATR: $0.66\n",
            "  Timeframe: SWING\n",
            "\n",
            "ðŸ’¡ Recommendation:\n",
            "  ðŸŸ¡ WATCH - Set alerts for entry\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ” **Cell 7: Ticker Lookup**\n",
        "#@markdown Enter any ticker for instant quality analysis.\n",
        "\n",
        "TICKER = \"GOOGL\" #@param {type:\"string\"}\n",
        "\n",
        "if TICKER.strip():\n",
        "    lookup_ticker(TICKER, DATA)\n",
        "else:\n",
        "    print(\"âŒ Please enter a ticker symbol\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx9Z6kew4Hsv",
        "outputId": "0d6b1607-5488-4163-9dab-11cc158a8d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸ“‹ DAILY TRADING BRIEF\n",
            "Thursday, February 05, 2026 | Pre-Market\n",
            "============================================================\n",
            "\n",
            "MARKET REGIME: Unable to fetch (The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().)\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "\n",
            "ðŸŽ¯ TOP 5 DAYTRADE SETUPS\n",
            "\n",
            "RKLB - B-TIER \n",
            "   Setup: Unknown | ATR: $8.21\n",
            "STRL - B-TIER \n",
            "   Setup: Unknown | ATR: $8.21\n",
            "DY - B-TIER \n",
            "   Setup: Unknown | ATR: $8.21\n",
            "NVMI - B-TIER \n",
            "   Setup: Unknown | ATR: $8.21\n",
            "FIVE - B-TIER \n",
            "   Setup: Unknown | ATR: $8.21\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "\n",
            "ðŸ“ˆ TOP 5 SWING SETUPS\n",
            "\n",
            "FIX - B-TIER \n",
            "   Setup: Unknown\n",
            "APLD - B-TIER \n",
            "   Setup: Unknown\n",
            "AEM - B-TIER \n",
            "   Setup: Unknown\n",
            "APH - B-TIER \n",
            "   Setup: Unknown\n",
            "AAOI - B-TIER \n",
            "   Setup: Unknown\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "\n",
            "ðŸ‘ï¸  WATCH LIST\n",
            "\n",
            "   No stocks on watch\n",
            "\n",
            "============================================================\n",
            "âœ… Daily Brief Complete\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ“‹ **Cell 8: Generate Daily Brief**\n",
        "#@markdown Generate comprehensive pre-market intelligence report.\n",
        "\n",
        "generate_daily_brief(DATA, top_daytrades=CONFIG['top_daytrades'], top_swings=CONFIG['top_swings'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "CfwrGWCD4Hsv",
        "outputId": "246f9415-bd92-445b-b3fd-7ba94293d903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Exported to canslim_brief_20260205.csv\n",
            "   Total: 498 stocks\n",
            "   A-TIER: 0\n",
            "   B-TIER: 160\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3d1cbbf9-24c2-40e8-8d59-b8088703e2f0\", \"canslim_brief_20260205.csv\", 117066)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title ðŸ“Š **Cell 9: Export Data**\n",
        "#@markdown Export the processed data to CSV.\n",
        "\n",
        "CUSTOM_FILENAME = \"\" #@param {type:\"string\"}\n",
        "DOWNLOAD_FILE = True #@param {type:\"boolean\"}\n",
        "\n",
        "filename = CUSTOM_FILENAME if CUSTOM_FILENAME.strip() else None\n",
        "exported_file = export_to_csv(DATA, filename)\n",
        "\n",
        "if exported_file and DOWNLOAD_FILE:\n",
        "    from google.colab import files\n",
        "    files.download(exported_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYm7sWAv4Hsv",
        "outputId": "3a7bb639-92ee-46a1-d4f3-65de3a21d7ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Quick commands loaded!\n",
            "\n",
            "Available commands:\n",
            "  â€¢ lookup_ticker('SYMBOL', DATA) - Quick ticker analysis\n",
            "  â€¢ show_a_tier() - View A-TIER stocks\n",
            "  â€¢ show_high_conviction() - Multi-source plays\n",
            "  â€¢ show_by_timeframe('DAYTRADE') - Filter by timeframe\n",
            "  â€¢ generate_daily_brief(DATA) - Full pre-market brief\n",
            "  â€¢ export_to_csv(DATA) - Export to file\n",
            "\n",
            "Examples:\n",
            "  lookup_ticker('AAPL', DATA)\n",
            "  show_by_timeframe('SWING')\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸŽ® **Cell 10: Quick Commands**\n",
        "#@markdown Utility functions for interactive use.\n",
        "\n",
        "def show_a_tier():\n",
        "    \"\"\"Display all A-TIER stocks\"\"\"\n",
        "    if DATA.ibd_universe.empty:\n",
        "        print(\"âŒ No data loaded\")\n",
        "        return\n",
        "\n",
        "    a_tier = DATA.ibd_universe[DATA.ibd_universe['Tier'] == 'A-TIER']\n",
        "\n",
        "    print(f\"\\n{'â”'*40}\")\n",
        "    print(f\"â­ A-TIER STOCKS ({len(a_tier)} total)\")\n",
        "    print(f\"{'â”'*40}\\n\")\n",
        "\n",
        "    if a_tier.empty:\n",
        "        print(\"No A-TIER stocks found\")\n",
        "    else:\n",
        "        for _, row in a_tier.iterrows():\n",
        "            print(f\"{row['Ticker']} - {row.get('Pivot_Status', 'N/A')} | {row.get('Timeframe', 'N/A')}\")\n",
        "\n",
        "\n",
        "def show_high_conviction():\n",
        "    \"\"\"Display high conviction (multi-source) stocks\"\"\"\n",
        "    if DATA.convergence_df.empty:\n",
        "        print(\"âŒ Run Cell 6 first to calculate convergence\")\n",
        "        return\n",
        "\n",
        "    high = DATA.convergence_df[DATA.convergence_df['Convergence_Score'] >= 8]\n",
        "\n",
        "    print(f\"\\n{'â”'*40}\")\n",
        "    print(f\"â­â­â­ HIGH CONVICTION ({len(high)} stocks)\")\n",
        "    print(f\"{'â”'*40}\\n\")\n",
        "\n",
        "    if high.empty:\n",
        "        print(\"No high conviction stocks (need multi-source confirmation)\")\n",
        "    else:\n",
        "        for _, row in high.iterrows():\n",
        "            tier = DATA.ticker_lookup_db.get(row['Ticker'], {}).get('Tier', 'Unknown')\n",
        "            print(f\"{row['Ticker']} - {tier} | Sources: {row['Sources']}\")\n",
        "\n",
        "\n",
        "def show_by_timeframe(timeframe: str):\n",
        "    \"\"\"Display stocks by timeframe (DAYTRADE, SWING, POSITION, WATCH)\"\"\"\n",
        "    if DATA.ibd_universe.empty or 'Timeframe' not in DATA.ibd_universe.columns:\n",
        "        print(\"âŒ Run Cell 6 first\")\n",
        "        return\n",
        "\n",
        "    tf = timeframe.upper()\n",
        "    stocks = DATA.ibd_universe[DATA.ibd_universe['Timeframe'] == tf]\n",
        "\n",
        "    print(f\"\\n{'â”'*40}\")\n",
        "    print(f\"{tf} CANDIDATES ({len(stocks)} total)\")\n",
        "    print(f\"{'â”'*40}\\n\")\n",
        "\n",
        "    for _, row in stocks.head(20).iterrows():\n",
        "        print(f\"{row['Ticker']} - {row.get('Tier', 'N/A')} | ATR: ${row.get('ATR', 'N/A')}\")\n",
        "\n",
        "    if len(stocks) > 20:\n",
        "        print(f\"\\n...and {len(stocks) - 20} more\")\n",
        "\n",
        "\n",
        "print(\"âœ… Quick commands loaded!\")\n",
        "print(\"\"\"\n",
        "Available commands:\n",
        "  â€¢ lookup_ticker('SYMBOL', DATA) - Quick ticker analysis\n",
        "  â€¢ show_a_tier() - View A-TIER stocks\n",
        "  â€¢ show_high_conviction() - Multi-source plays\n",
        "  â€¢ show_by_timeframe('DAYTRADE') - Filter by timeframe\n",
        "  â€¢ generate_daily_brief(DATA) - Full pre-market brief\n",
        "  â€¢ export_to_csv(DATA) - Export to file\n",
        "\n",
        "Examples:\n",
        "  lookup_ticker('AAPL', DATA)\n",
        "  show_by_timeframe('SWING')\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX-0qFDg4Hsv"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ“š Quick Reference\n",
        "\n",
        "### Tier Classification\n",
        "\n",
        "| Tier | Criteria | Action |\n",
        "|------|----------|--------|\n",
        "| A-TIER | RS â‰¥ 90, Composite â‰¥ 90, Proper Setup | Full position |\n",
        "| B-TIER | RS â‰¥ 80, Composite â‰¥ 80 | Smaller position |\n",
        "| C-TIER | RS â‰¥ 70 OR Composite â‰¥ 70 | Daytrade only |\n",
        "| D-TIER | Below thresholds | Pass |\n",
        "\n",
        "### Convergence Scoring\n",
        "\n",
        "| Stars | Score | Meaning |\n",
        "|-------|-------|----------|\n",
        "| â­â­â­ | 8-10 | High conviction (multiple quality sources) |\n",
        "| â­â­ | 6-7 | Medium conviction |\n",
        "| â­ | 4-5 | Low conviction |\n",
        "| - | 0-3 | Single source only |\n",
        "\n",
        "### Timeframe Classification\n",
        "\n",
        "| Timeframe | Criteria | Holding Period |\n",
        "|-----------|----------|----------------|\n",
        "| DAYTRADE | ATR > $5, within 5% of pivot | Intraday |\n",
        "| SWING | Within 5% of pivot | 2-5 days |\n",
        "| POSITION | Within 10% of pivot | 1-4 weeks |\n",
        "| WATCH | Extended from pivot | Wait for setup |\n",
        "\n",
        "### Source Weights (Default)\n",
        "\n",
        "| Source | Weight | Description |\n",
        "|--------|--------|-------------|\n",
        "| IBD 50 | 3 | Top institutional quality |\n",
        "| IBD 250 | 2 | Growth leaders |\n",
        "| DeepVue | 2 | True market leaders |\n",
        "| Primus | 2 | Pre-market momentum |\n",
        "| Finviz | 1 | Technical scans |\n",
        "| X/Twitter | 1 | Social mentions |\n",
        "| Watchlist | 1 | Personal picks |\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ˆ Daily Workflow\n",
        "\n",
        "1. **Pre-market (5 min)**\n",
        "   - Upload fresh IBD exports (Cell 4)\n",
        "   - Add Primus/DeepVue tickers (Cell 5)\n",
        "   - Process universe (Cell 6)\n",
        "\n",
        "2. **Generate Brief (instant)**\n",
        "   - Run Cell 8 for daily brief\n",
        "   - Focus on top daytrades + swings\n",
        "\n",
        "3. **During Market**\n",
        "   - Use Cell 7 for quick lookups\n",
        "   - Trust A-TIER + â­â­â­ convergence\n",
        "\n",
        "4. **Post-market**\n",
        "   - Export results (Cell 9)\n",
        "   - Review for next day\n",
        "\n",
        "---\n",
        "\n",
        "*Built with the standardized notebook template | Version 7.0*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}